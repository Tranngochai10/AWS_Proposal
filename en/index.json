[{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Python 3.13 runtime is now available in AWS Lambda by Julian Wood | on 14 NOV 2024 | in Announcements, AWS Cloud Development Kit, AWS Lambda, AWS SDK for Python, AWS Serverless Application Model, Python, Serverless | Permalink | Share\nThis post is written by Julian Wood, Principal Developer Advocate, and Leandro Cavalcante Damascena, Senior Solutions Architect Engineer.\nAWS Lambda now supports Python 3.13 as a managed runtime and container base image. Python is a popular language for building serverless applications. The Python 3.13 release includes a number of changes to the language, implementation, and standard library. With this release, Python developers can now take advantage of these new features and improvements when creating serverless applications on Lambda. Python 3.13 also includes experimental support for some features, but these features are not available in Lambda.\nYou can develop Lambda functions in Python 3.13 using the AWS Management Console, AWS Command Line Interface (AWS CLI), AWS SDK for Python (Boto3), AWS Serverless Application Model (AWS SAM), AWS Cloud Development Kit (AWS CDK), and other infrastructure as code tools.\nThe Python 3.13 runtime allows you to implement serverless best practices using Powertools for AWS Lambda (Python). This is a developer toolkit that includes observability, batch processing, AWS Systems Manager Parameter Store integration, idempotency, feature flags, Amazon CloudWatch Metrics, structured logging, and more.\nLambda@Edge allows you to use Python 3.13 to customize low-latency content delivered through Amazon CloudFront.\nLambda runtime changes Amazon Linux 2023 Like the Python 3.12 runtime, the Python 3.13 runtime is based on the provided.al2023 runtime, which is built on the Amazon Linux 2023 minimal container image. The Amazon Linux 2023 minimal image uses microdnf as the package manager, which is symlinked as dnf. This replaces the yum package manager used in Python 3.11 and earlier AL2-based images.\nIf you deploy your Lambda functions as container images, you must update your Dockerfiles to use dnf instead of yum when upgrading to the Python 3.13 base image from Python 3.11 or earlier base images.\nLearn more about the provided.al2023 runtime in the blog post Introducing the Amazon Linux 2023 runtime for AWS Lambda and the Amazon Linux 2023 launch blog post.\nNew Python features Data model improvements There are improvements to the Python data model. static_attributes stores the names of attributes accessed through self.X in any function in a class body.\nTyping changes With the implementation of PEP 702, you can now use the new warnings.deprecated() decorator to mark deprecations in the type system and at runtime.\nPython 3.13 also adds PEP 696, which introduces default values for type parameters. This improvement allows developers to specify default types for TypeVar, ParamSpec, and TypeVarTuple when omitting type arguments.\nStandard library The standard library includes improvements for a new PythonFinalizationError exception, which is raised when an operation is blocked during finalization.\nNew functions base64.z85encode() and base64.z85decode() support encoding and decoding Z85 data.\nThe copy module now has a copy.replace() function, with support for many built-in types and any class that defines the replace() method.\nThe os module has a suite of new functions for working with Linux timer notification file descriptors.\nThere is a change to the mutation semantics defined for locals().\nUnavailable experimental features Python 3.13 includes some experimental features that are not enabled for Lambda managed runtime or base images. These features must be enabled when the Python runtime is compiled. Because the Lambda-provided Python 3.13 runtime is designed for production workloads, these features are not enabled in Lambda\u0026rsquo;s build of Python 3.13 and cannot be enabled through execution-time flags.\nTo use these features in Lambda, you can deploy your own Python runtime using a custom runtime or container image with these features enabled.\nFree-threaded CPython You cannot enable experimental support to run Python in free-threaded mode, with the global interpreter lock (GIL) disabled.\nJust-in-time (JIT) compiler You also cannot enable the experimental JIT compiler in Lambda managed runtime or base image.\nPerformance considerations At launch, new Lambda runtimes receive less usage compared to existing established runtimes. This can lead to longer cold start times due to reduced cache residency in internal Lambda sub-systems. Cold start times are typically improved in the weeks following launch as usage increases.\nTherefore, AWS recommends not drawing conclusions from performance comparisons side-by-side with other Lambda runtimes until performance has stabilized. Since performance depends heavily on the workload, customers with performance-sensitive workloads should conduct their own testing, rather than relying on general benchmark tests.\nUsing Python 3.13 in Lambda AWS Management Console To use the Python 3.13 runtime to develop your Lambda functions, specify the runtime parameter value as Python 3.13 when creating or updating a function.\nPython version 3.13 is available in the Runtime dropdown on the Create Function page.\nTo update an existing Lambda function to Python 3.13, navigate to the function in the Lambda console and select Edit in the Runtime settings panel. The new Python version is available in the Runtime dropdown.\nYou may need to check your code and dependencies to ensure compatibility with Python 3.13, and update as necessary.\nAWS Lambda container image Change the Python base image version by modifying the FROM statement in your Dockerfile.\nFROM public.ecr.aws/lambda/python:3.13 # Copy function code COPY lambda_handler.py ${LAMBDA_TASK_ROOT} "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Implementing custom domain names for private endpoints with Amazon API Gateway by Chris McPeek | on 21 NOV 2024 | in Amazon API Gateway, Amazon EC2, Amazon Elastic Container Service, Amazon Elastic Kubernetes Service, AWS Lambda, Resource Access Manager (RAM), Serverless | Permalink | Share\nThis post is written by Heeki Park, Principal Solutions Architect\n1/23/25: This post has been updated to correct the AWS CloudFormation templates.\nAmazon API Gateway is introducing custom domain name support for private REST API endpoints. Customers choose private REST API endpoints when they want endpoints that can only be invoked from within their Amazon VPC. Custom domain names are simpler and more intuitive URLs that you can use with your application and were previously only supported with public REST API endpoints. Now you can use custom domain names to map to private REST APIs and share those custom domain names across accounts using AWS Resource Access Manager (AWS RAM).\nOverview of API Gateway connectivity When considering network connectivity with API Gateway, there are two important aspects to note: integration type and connectivity type. The following diagram shows examples of those considerations.\nFigure 1: Overall architecture\nThe first aspect is the distinction between frontend integrations and backend integrations. Frontend integrations are how API clients such as mobile devices, web browsers, or client applications connect to the API endpoint. Backend integrations are the API services that your API Gateway endpoint proxies requests to, such as applications running on Amazon Elastic Compute Cloud (EC2) instances, Amazon Elastic Kubernetes Service (EKS) or Amazon Elastic Container Service (ECS) containers, or as AWS Lambda functions. The second aspect is whether that connection goes through the public internet or through your private VPC.\nInvoking private REST API endpoints To send requests to a private REST API endpoint, clients must operate in a VPC configured with a VPC endpoint. When the VPC endpoint is configured, a client has three different options in the VPC to connect to the API endpoint, depending on how the VPC and VPC endpoint are configured.\nIf the VPC endpoint has private DNS enabled, the client can send requests to the standard endpoint URL: https://{api-id}.execute-api.{region}.amazonaws.com/{stage}. These requests will resolve to the VPC endpoint, which is then routed to the appropriate API Gateway endpoint.\nFigure 2: VPC endpoint configured with private DNS names enabled\nAlternatively, if the VPC endpoint has private DNS disabled, the client can send requests to the VPC endpoint URL: https://{vpce-id}.execute-api.{region}.amazonaws.com/{stage}. One of the following headers also needs to be sent with that request.\nHost: {api-id}.execute-api.us-east-1.amazonaws.com\nx-apigw-api-id: {api-id}\nFinally, if the VPC endpoint has private DNS disabled and the private REST API endpoint is associated with the VPC endpoint, the client can send requests to the following URL: https://{api-id}-{vpce-id}.execute-api.{region}.amazonaws.com/{stage}. To associate a VPC endpoint with a private API, the following property configures that association.\nEndpointConfiguration: Type: PRIVATE VPCEndpointIds: - !Ref vpcEndpointId "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Accelerating data science innovation: How Bayer Crop Science uses AWS AI/ML services to build their next-generation MLOps service by Lance Smith, Jake Malmad, Karthik Prabhakar, Kenton Blacutt, and Nicole Brown on 08 JUL 2025 in Amazon API Gateway, Amazon EventBridge, Amazon Q, Amazon Q Business, Amazon Q Developer, Amazon SageMaker, Amazon SageMaker Data \u0026amp; AI Governance, Artificial Intelligence, Customer Solutions, Generative AI | Permalink | Comments | Share\nThe world\u0026rsquo;s population is growing at a rapid pace. The growing global population requires innovative solutions to produce food, fiber, and fuel, while restoring natural resources like soil and water and addressing climate change. Bayer Crop Science estimates that farmers need to increase crop yields by 50% by 2050 to meet these demands. To support their mission, Bayer Crop Science is partnering with farmers and partners to promote and scale regenerative agriculture—a future where farming can produce more while restoring the environment.\nRegenerative agriculture is a sustainable farming philosophy aimed at improving soil health by integrating nature to create healthy ecosystems. It\u0026rsquo;s based on the idea that agriculture should restore degraded land and reverse degradation, rather than maintaining current conditions. The Crop Science division at Bayer believes that regenerative agriculture is the foundation for the future of farming. Their vision is to produce 50% more food by restoring nature and scaling regenerative agriculture. To make this mission a reality, Bayer Crop Science is accelerating model training with Amazon SageMaker and speeding up code documentation with Amazon Q.\nIn this post, we present how Bayer Crop Science manages large-scale data science operations by training models for their data analysis needs and maintaining high-quality code documentation to support developers. Through these solutions, Bayer Crop Science expects to reduce developer onboarding time by up to 70% and improve developer productivity by up to 30%.\nThe challenge Bayer Crop Science faced the challenge of scaling genomic prediction models to accelerate time to market. The company also needed data scientists to focus on building high-value foundation models (FMs), rather than worrying about building and designing solutions. Before building their Decision Science Ecosystem solution, provisioning a data science environment could take many days for a data team within Bayer Crop Science.\nSolution overview Bayer Crop Science\u0026rsquo;s Decision Science Ecosystem (DSE) is a next-generation machine learning operations (MLOps) solution built on AWS to accelerate data-driven decision-making for data science teams at scale across the organization. AWS services support Bayer Crop Science in creating a connected decision-making system accessible to thousands of data scientists. The company is using the solution for generative AI, advances in product pipeline, geospatial image analysis of field data, and large-scale genomic prediction models that will enable Bayer Crop Science to become more data-driven and accelerate time to market. This solution supports data scientists at every step, from idea to model output, including the complete record of business decisions made using DSE. Other divisions within Bayer are also beginning to build a similar solution on AWS based on DSE\u0026rsquo;s success.\nBayer Crop Science teams\u0026rsquo; DSE integrates seamlessly with SageMaker, a fully managed service that enables data scientists to quickly build, train, and deploy machine learning (ML) models for different use cases so they can make data-driven decisions quickly. This drives collaboration within Bayer Crop Science between product supply, R\u0026amp;D, and commercial. Their data science strategy no longer requires self-service data engineering, but instead provides an efficient resource to drive fast data engineering at scale. Bayer Crop Science chose SageMaker because it provides a single cohesive experience where data scientists can focus on building high-value models, without having to worry about building and designing that resource itself. With the help of AWS services, cross-functional teams can quickly align to reduce operational costs by minimizing redundancy, addressing errors early and often, and quickly identifying issues in automated workflows. The DSE solution uses SageMaker, Amazon Elastic Kubernetes Service (Amazon EKS), AWS Lambda, and Amazon Simple Storage Service (Amazon S3) to accelerate innovation at Bayer Crop Science and create a seamless, customized end-to-end user experience.\nThe following diagram illustrates the DSE architecture.\nSolution walkthrough Bayer Crop Science had two main challenges in managing large-scale data science operations: maintaining high-quality code documentation and optimizing existing documentation across multiple repositories. With Amazon Q, Bayer Crop Science addressed both challenges, helping them onboard developers faster and improve developer productivity.\nThe company\u0026rsquo;s first use case focused on automatically generating high-quality code documentation. When a developer pushes code to a GitHub repository, a webhook—a lightweight, event-driven communication that automatically sends data between applications using HTTP—triggers a Lambda function through Amazon API Gateway. This function then uses Amazon Q to analyze code changes and generate comprehensive documentation as well as change summaries. The updated documentation is then stored in Amazon S3. The same Lambda function also creates a pull request with an AI-generated summary of code changes. To maintain security and flexibility, Bayer Crop Science uses Parameter Store, a capability of AWS Systems Manager, to manage prompts for Amazon Q, allowing quick updates without redeployment, and AWS Secrets Manager to securely handle repository tokens.\nThis automation significantly reduces the time developers spend creating documentation and pull request descriptions. The generated documentation is also fed into Amazon Q, so developers can quickly answer questions they have about a repository and onboard into projects.\nThe second use case addresses the challenge of maintaining and improving the quality of existing code documentation. An AWS Batch job, triggered by Amazon EventBridge, processes code repositories. Amazon Q generates new documentation for each code file, which is then indexed along with the source code. The system also creates high-level documentation for each module or function and compares AI-generated documentation with existing human-written documentation. This process allows Bayer Crop Science to systematically evaluate and enhance their documentation quality over time.\nTo improve searchability, Bayer Crop Science added repository names as custom attributes in the Amazon Q index and prefixed them to indexed content. This improvement enhances the accuracy and relevance of documentation searches. The development team also implemented strategies to handle API throttling and variability in AI responses, maintaining robustness in the production environment. Bayer Crop Science is considering developing a management plane to streamline adding new repositories and centralize management of settings, tokens, and prompts. This will continue to enhance the system\u0026rsquo;s scalability and ease of use.\nOrganizations wanting to replicate Bayer Crop Science\u0026rsquo;s success can implement similar webhook-triggered documentation generation, use Amazon Q Business for both documentation generation and quality evaluation, and integrate the solution with existing version control and code review processes. By using AWS services like Lambda, Amazon S3, and Systems Manager, companies can create a scalable and manageable architecture for their documentation needs. Amazon Q Developer also helps organizations further accelerate their development timelines by providing real-time code suggestions and a next-generation chat experience built-in.\n\u0026ldquo;One of the lessons we\u0026rsquo;ve learned over the past 10 years is that we want to write less code. We want to focus our time and investment only on things that bring differential value to Bayer, and we want to leverage everything that AWS provides out of the box. Part of our goal is to reduce the development cycles needed to move a model from proof-of-concept stage, to production, and finally to business adoption. That\u0026rsquo;s where the value is.\u0026rdquo;\n– Will McQueen, VP, Head of CS Global Data Assets and Analytics at Bayer Crop Science.\nConclusion Bayer Crop Science\u0026rsquo;s approach aligns with modern MLOps practices, enabling data science teams to focus more on high-value modeling tasks rather than time-consuming documentation processes and infrastructure management. By adopting these practices, organizations can significantly reduce the time and effort required for code documentation while improving overall code quality and team collaboration.\nLearn more about Bayer Crop Science\u0026rsquo;s generative AI journey, and explore how Bayer Crop Science is redesigning sustainable practices through advanced technology.\nAbout Bayer Bayer is a global enterprise with core competencies in the life science fields of healthcare and nutrition. In line with its mission, \u0026ldquo;Health for all, Hunger for none,\u0026rdquo; the company\u0026rsquo;s products and services are designed to help people and the planet thrive by supporting efforts to understand the major challenges posed by growing and aging global populations. Bayer is committed to driving sustainable development and creating a positive impact with its business activities. At the same time, Bayer aims to increase its earning power and create value through innovation and growth. The Bayer brand represents trust, reliability, and quality worldwide. In fiscal year 2023, the Group had approximately 100,000 employees and had sales of 47.6 billion euros. R\u0026amp;D expenses before special items amounted to 5.8 billion euros. For more information, visit www.bayer.com.\nAbout the authors Lance Smith is a Senior Solutions Architect and is part of the global Healthcare and Life Sciences industry division at AWS. He has spent the past 2 decades helping life sciences companies adopt technology to pursue their mission of helping their patients. Outside of work, he enjoys traveling, hiking, and spending time with family.\nKenton Blacutt is an AI Consultant in the Amazon Q Customer Success team. He works directly with customers, helping them solve real business problems with cutting-edge AWS technologies. In his free time, he enjoys traveling and occasionally running marathons.\nKarthik Prabhakar is a Senior Applications Architect in the AWS Professional Services team. In this role, he collaborates with customers to design and implement cutting-edge solutions for their critical business systems, focusing on areas like scalability, reliability, and cost optimization in digital transformation and modernization projects.\nJake Malmad is a Senior DevOps Consultant in the AWS Professional Services team, specializing in infrastructure as code, security, containers, and orchestration. As a DevOps consultant, he uses this expertise to collaborate with customers, designing and implementing solutions for automation, scalability, reliability, and security across various types of cloud adoption and transformation projects.\nNicole Brown is a Senior Engagement Manager in the AWS Professional Services team based in Minneapolis, MN. With over 10 years of professional expertise, she has led cross-functional, global teams in the healthcare and life sciences industries. She is also an advocate for women in technology and currently holds a board position in the Women at Global Services affinity group.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “VietNam Cloud Day 2025 – Ho Chi Minh City Connect Edition for Builders” Event Objectives Share insights on the latest trends in cloud computing and digital transformation Connect cloud professionals, developers, and business leaders in Vietnam Showcase real-world use cases of AWS Cloud services for startups and enterprises Provide technical sessions for builders to explore cloud-native development and AI integration Speakers AWS Vietnam Solution Architects – Technical experts presenting modernization strategies Industry Partners \u0026amp; Builders – Sharing real-world cloud migration success stories Guest Speakers – Local innovators and startup founders using AWS technologies Key Highlights 1. Cloud Transformation in Vietnam Overview of how cloud adoption is accelerating across industries Shared success stories from companies modernizing with AWS Emphasis on cloud security, scalability, and cost optimization 2. Building with AWS Hands-on sessions on deploying serverless applications with AWS Lambda and API Gateway Introduction to Amazon S3, EC2, and CloudFront for web applications Demonstration of infrastructure automation using AWS CloudFormation 3. Generative AI and Modern Development Discussion on how AI and machine learning are integrated into development workflows Use cases with Amazon Bedrock and CodeWhisperer for AI-assisted development Highlighted potential of GenAI for accelerating innovation 4. Networking and Collaboration Opportunities to connect with AWS partners, developers, and solution architects Experience-sharing among cloud builders and technology enthusiasts Discussion on career opportunities and community initiatives Key Takeaways Technical Knowledge Improved understanding of AWS core services and cloud architecture best practices Learned how to design scalable, secure, and cost-efficient systems using AWS tools Understood the benefits of serverless, containerized, and event-driven architectures Professional Development Enhanced communication and networking skills through discussions with experts Gained awareness of the future direction of cloud and AI technologies Learned from real-world implementation challenges and solutions Applying to Work Apply cloud principles to current or future projects (e.g., modular, event-driven design) Use AWS Free Tier for self-learning and experimentation with services like Lambda and S3 Share insights with team members about security, scalability, and cost optimization in the cloud Join local AWS Builder Community to continue learning and connecting with professionals Event Experience Attending VietNam Cloud Day 2025 – Ho Chi Minh City Connect Edition for Builders was an inspiring and educational experience.\nLearning and Inspiration The sessions provided deep insight into how AWS is shaping the future of technology in Vietnam. I learned directly from AWS engineers and business experts about modern architecture and AI trends. Hands-on Demonstrations Experienced practical demos on deploying serverless applications and automating infrastructure. Explored the use of GenAI tools to streamline the software development process. Networking and Community Engaged in meaningful conversations with developers, architects, and startup founders. Gained useful advice about career growth in the cloud and AI ecosystem. Lessons Learned Cloud adoption requires both technical readiness and mindset change within teams. Collaboration between business and technology teams is essential for successful transformation. Continuous learning and community participation are key to keeping up with cloud innovation. Event photos "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “[AWS GenAI Builder Club] AI-Driven Development Life Cycle: Reimagining Software Engineering” Event Objectives Introduce the concept of AI-Driven Development and how Artificial Intelligence is transforming modern software engineering Demonstrate the integration of Generative AI (GenAI) throughout the software development lifecycle (SDLC) Present AI tools that support developers in coding, testing, and maintenance Strengthen the AWS GenAI Builder Club Vietnam community by connecting developers and cloud enthusiasts Speakers AWS Vietnam Experts – Presented GenAI solutions and tools for developers Solution Architects – Shared real-world examples of applying AI in software engineering Guest Speakers – Startup founders and engineers discussing their hands-on experience with AI-driven development Key Highlights 1. Introduction to AI-Driven Development Defined the concept of AI-driven software development and its real-world implications Explained how AI enhances every stage of SDLC — from planning, design, coding, and testing to maintenance Emphasized AI’s role in automation, creativity, and developer productivity 2. Tools and Practical Demonstrations Showcased AWS tools such as Amazon CodeWhisperer and Amazon Q Developer Demonstrated how AI assists in code generation, bug detection, and performance optimization Introduced the concept of AI Pair Programming, where AI collaborates with developers in real time 3. Automating the Development Lifecycle Applied GenAI to CI/CD pipelines for automated build, test, and deployment Introduced AI agents for software project management and system monitoring Shared use cases of AI-driven DevOps, enabling faster releases with fewer errors 4. Security and Responsible AI Discussed data privacy, AI safety, and model transparency Introduced the Responsible AI Framework promoted by AWS Highlighted the balance between automation and human oversight in AI-assisted development 5. Community and Networking Engaged in open discussions between speakers and participants Encouraged collaboration and experience sharing among AWS Builders and AI developers Fostered networking opportunities for career growth in the AI and cloud ecosystem Key Takeaways Technical Knowledge Gained a deeper understanding of how AI supports developers across the entire SDLC Learned to use AWS GenAI tools like CodeWhisperer, Amazon Q Developer, and Bedrock Understood how to integrate AI into DevOps pipelines for automation and efficiency Professional Development Learned to treat AI as a “technical teammate” that enhances creativity and productivity Gained awareness of ethical and responsible AI practices Expanded my professional network through discussions with AWS experts and developers Applying to Work Apply AI-assisted coding in personal and professional projects Integrate Generative AI in testing and maintenance workflows to boost productivity Experiment with AWS tools like CodeWhisperer and Amazon Q Developer for real-world development Propose the adoption of AI-driven automation in the development process of my current team or internship Event Experience Attending [AWS GenAI Builder Club] AI-Driven Development Life Cycle: Reimagining Software Engineering was an inspiring and eye-opening experience that deepened my understanding of how AI is reshaping software development.\nLearning and Insights Learned how AI is transforming traditional development practices and influencing the software lifecycle. Understood how AI-driven tools can reduce manual work, minimize errors, and accelerate delivery speed. Discovered how AI empowers developers to focus more on creativity and design rather than repetitive tasks. Hands-on Exploration Experienced live demonstrations using Amazon CodeWhisperer and Amazon Q Developer. Learned how to use AI tools for faster, more accurate code generation while maintaining best security practices. Networking and Community Engaged with AWS specialists, engineers, and startup founders to exchange knowledge and experiences. Gained insights into career development in AI and Cloud Computing fields. Lessons Learned AI is not just a tool — it is a strategic collaborator in the modern software engineering process. Responsible AI usage requires human judgment, transparency, and ethical practices. Continuous learning and experimentation are essential to fully leverage AI capabilities in software development. Conclusion: This event expanded my technical perspective on AI-driven development and inspired me to explore how Generative AI can be integrated into real-world software engineering practices to enhance innovation and efficiency.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: “[AWS GenAI Builders Club] Game Day – Secret Agent(ic) Unicorns” Event Objectives Provide participants with a hands-on experience in solving real-world cloud challenges using AWS services Strengthen problem-solving, teamwork, and time management skills through interactive, mission-based activities Encourage developers to explore event-driven architecture, automation, and system monitoring in practical scenarios Promote collaboration and knowledge-sharing within the AWS Builders Club Vietnam community Event Format and Overview The Game Day was a team-based simulation event where participants acted as “cloud agents” completing a series of technical missions.\nEach team managed a simulated environment on AWS, resolving challenges related to infrastructure performance, cost optimization, and fault tolerance.\nThe game followed a storyline of “Secret Agent(ic) Unicorns,” combining technical problem-solving with creativity and fun.\nThe competition required quick thinking, teamwork, and mastery of AWS tools to achieve the best score within the time limit.\nKey Highlights 1. Team Collaboration and Strategy Participants worked in small teams to design, deploy, and maintain cloud systems under time pressure Required strong communication and coordination between team members Encouraged planning and task distribution to maximize efficiency and minimize downtime 2. Real-Time Cloud Problem Solving Teams were challenged to fix system outages, security misconfigurations, and cost overruns Applied AWS CloudWatch, EC2, Lambda, and S3 to monitor and optimize infrastructure Implemented auto-scaling policies and event-driven functions to improve system reliability 3. Learning Through Play The game format created an engaging way to learn complex cloud concepts Mistakes became learning opportunities, helping participants understand real-world troubleshooting Reinforced the value of resilience, automation, and proactive monitoring in system design 4. AWS Tools and Technologies Used Amazon CloudWatch – Real-time monitoring and alerting AWS Lambda – Automating event-driven tasks Amazon EC2 – Managing compute instances Amazon S3 – Storage and backup solutions CloudFormation – Infrastructure automation and reproducibility Key Takeaways Technical Learning Gained practical experience in managing AWS cloud environments under real constraints Understood how to respond to incidents, optimize performance, and reduce costs Strengthened familiarity with monitoring, logging, and automation tools in AWS Teamwork and Soft Skills Improved collaboration, leadership, and communication skills through team-based challenges Learned how to stay calm and organized when dealing with multiple issues simultaneously Experienced the importance of strategic decision-making under pressure Applying to Work Apply cloud troubleshooting and automation techniques learned during the event in real projects Implement monitoring and alerting systems similar to AWS CloudWatch in development environments Encourage team-based learning and hands-on problem-solving sessions for continuous skill improvement Share knowledge with peers about event-driven systems and incident response best practices Event Experience Attending [AWS GenAI Builders Club] Game Day – Secret Agent(ic) Unicorns was one of the most exciting and practical learning experiences of my internship.\nLearning and Engagement The gamified environment made learning about AWS tools fun and interactive. I learned to manage real-time cloud issues while maintaining teamwork and focus. The competition atmosphere pushed me to think critically and make quick, data-driven decisions. Hands-on Technical Practice Worked directly with AWS CloudWatch, Lambda, and EC2 to manage infrastructure issues. Applied automation scripts to stabilize systems during performance peaks. Experienced firsthand how event-driven systems can improve resilience and efficiency. Team Collaboration Collaborated closely with my teammates to prioritize tasks and solve incidents efficiently. Realized the importance of clear communication and documentation in fast-paced technical environments. Lessons Learned Practical cloud operations require both technical expertise and teamwork. Monitoring and automation are essential for maintaining system stability. Continuous learning through real-world simulations enhances both confidence and competence. Event Photos Conclusion: This event provided a unique opportunity to apply cloud knowledge in a real-time, competitive setting. It strengthened my technical, analytical, and teamwork skills while deepening my understanding of AWS operations and event-driven architectures.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Tran Ngoc Hai\nPhone Number: 0934093061\nEmail: Haitnse182387@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 9/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Workshop Overview What is Serverless? Serverless computing is a cloud computing model where the cloud provider manages the infrastructure, automatically provisioning, scaling, and managing servers. You only pay for the compute time you consume.\nArchitecture Overview In this workshop, we\u0026rsquo;ll build a serverless REST API with the following architecture:\nClient Request ↓ API Gateway (REST API) ↓ Lambda Function (Business Logic) ↓ DynamoDB (Data Storage) Components 1. Amazon API Gateway Creates RESTful APIs Handles HTTP requests and responses Manages authentication and authorization Provides throttling and monitoring 2. AWS Lambda Serverless compute service Executes code in response to events Automatically scales based on traffic Pay only for compute time used 3. Amazon DynamoDB NoSQL database service Fully managed and serverless Fast and scalable Automatic scaling What We\u0026rsquo;ll Build A Task Management API with the following endpoints:\nPOST /tasks - Create a new task GET /tasks - List all tasks GET /tasks/{id} - Get a specific task PUT /tasks/{id} - Update a task DELETE /tasks/{id} - Delete a task Benefits of Serverless Architecture No server management - AWS handles all infrastructure Automatic scaling - Handles traffic spikes automatically Cost-effective - Pay only for what you use High availability - Built-in redundancy and fault tolerance Fast development - Focus on code, not infrastructure "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":" Week 1 Objectives: Connect and get acquainted with First Cloud Journey team members Understand basic AWS services, how to use Console \u0026amp; CLI Learn cost optimization strategies Explore and configure IAM Create Groups and Users, and establish connection from local machine to cloud Understand cloud layer architecture Tasks to be implemented this week: Day Task Start Date Completion Date Documentation Source 2 - Get acquainted with FCJ team members - Read and understand company policies and internship regulations 09/09/2025 09/09/2025 3 - Learn about AWS and service categories + Compute + Storage + Networking + Database + Security \u0026amp; Identity 12/08/2025 12/08/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn AWS Console \u0026amp; AWS CLI - Hands-on Practice: + Create AWS account + Install and configure AWS CLI + Learn how to use AWS CLI 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn EC2 basics: + Instance types + AMI (Amazon Machine Images) + EBS (Elastic Block Store) + Security Groups - Methods to SSH remote into EC2 - Learn about Elastic IP 14/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 - Hands-on Practice: + Create EC2 instance + Establish SSH connection + Mount EBS volume + Configure Security Groups 15/08/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understanding of AWS and core service categories:\nCompute: EC2, Lambda, ECS Storage: S3, EBS, EFS Networking: VPC, CloudFront, Route 53 Database: RDS, DynamoDB Security: IAM, WAF Successfully created and configured AWS Free Tier account.\nFamiliarized with AWS Management Console and learned how to find, access, and use services through web interface.\nInstalled and configured AWS CLI on local machine including:\nAccess Key ID Secret Access Key Default region (ap-southeast-1) Output format (json) Used AWS CLI to perform basic operations such as:\nCheck account information \u0026amp; configuration: aws sts get-caller-identity Get list of regions: aws ec2 describe-regions View EC2 services: aws ec2 describe-instances Create and manage key pairs: aws ec2 create-key-pair Check running service information Gained ability to seamlessly switch between web interface and CLI for AWS resource management.\nUnderstanding of cloud layer architecture:\nData Centers: Physical data center facilities Availability Zones (AZ): Include one or more data centers, minimum 2 data centers connected by high-speed networks Regions: Minimum 3 AZs per region, geographically distributed Edge Locations: Secondary network to run edge network services Main edge network services commonly used in Vietnam:\nCloudFront (CDN): Accelerate content access speed Web Application Firewall (WAF): Creates protective layer to prevent DDoS attacks Route 53 (DNS service): Create domain for applications and attach SSL certificates Cost optimization strategies:\nSelect appropriate compute resources and storage configurations based on actual needs Avoid running full configuration when unnecessary Use Free Tier for learning and experimentation Turn off/delete unused resources to save costs "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/1-worklog/","title":"Worklog","tags":[],"description":"","content":" On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Employee Management Web Project - Requirements analysis and system design\nWeek 8: Develop Employee Login page with SSO and MFA\nWeek 9: Complete Login integration and start Manager Dashboard development\nWeek 10: Employee Management module - List, Detail, Search and Filter\nWeek 11: Leave Management and Approval workflow, Attendance tracking\nWeek 12: Reports \u0026amp; Analytics, Performance optimization and Testing\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/5-workshop/5.2-prerequiste/","title":"Prerequisites and Setup","tags":[],"description":"","content":"Prerequisites Required AWS Services Access To complete this workshop, you need access to the following AWS services:\nAWS Lambda Amazon API Gateway Amazon DynamoDB AWS IAM Amazon CloudWatch Logs IAM Permissions Your AWS account needs the following IAM permissions:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:*\u0026#34;, \u0026#34;apigateway:*\u0026#34;, \u0026#34;dynamodb:*\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Software Requirements AWS Account with appropriate permissions AWS Console Access or AWS CLI configured Basic knowledge of Python 3.x Text editor or IDE (VS Code, PyCharm, etc.) AWS Region We\u0026rsquo;ll use us-east-1 (N. Virginia) for this workshop. You can choose any region, but make sure all resources are created in the same region.\nSetup Steps Step 1: Verify AWS Access Log in to the AWS Management Console Navigate to the AWS Lambda service Ensure you can see the Lambda dashboard Step 2: Choose Your Region Select your preferred AWS region from the top-right corner Remember this region - all resources will be created here Step 3: Prepare Your Environment Have your text editor ready for writing Lambda function code Keep the AWS Console open in your browser Optionally, have AWS CLI configured if you prefer command-line tools Estimated Costs This workshop uses AWS Free Tier eligible services:\nLambda: 1M free requests per month API Gateway: 1M API calls per month for REST APIs DynamoDB: 25GB storage, 25 read/write capacity units Note: If you exceed free tier limits, you may incur minimal charges. Always clean up resources after completing the workshop.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Enterprise HR Management System Comprehensive HR Management Solution for Modern Enterprises 1. Executive Summary Enterprise HR Management System is an integrated HR management solution designed for mid-sized enterprises in Vietnam, supporting 100-500 employees. The system automates the entire HR workflow from profile management, attendance tracking, payroll calculation to performance evaluation. This is an in-house project developed by the team, focusing on MVP with optimized costs under $100/month in the initial phase (100 employees), utilizing AWS serverless architecture with Lambda, API Gateway, DynamoDB to ensure high performance and low costs.\n2. Problem Statement Current Issues Vietnamese enterprises use Excel or legacy HR software, causing time waste and errors. Manual processes (attendance, payroll) are not integrated. No automated approval workflows. Difficult to manage detailed permissions. Weak reporting, not real-time. High costs for SAP, Workday solutions. Proposed Solution The system uses AWS Serverless Architecture to optimize costs:\nCompute: AWS Lambda (pay-per-use, no idle costs). API: API Gateway REST API. Database: DynamoDB (on-demand billing). Cache: ElastiCache Redis (cache.t3.micro) - optional for phase 2. Authentication: AWS Cognito (free tier \u0026lt;50K MAU). Storage: S3 for documents, CloudFront CDN. CI/CD: GitHub Actions for automated deployment. Monitoring: CloudWatch (free tier). Security: Route 53, WAF (cost-optimized rules), IAM Roles. Key Features Single Sign-On (Google, Microsoft 365). Detailed RBAC (Admin, Manager, Employee, Payroll Officer). Check-in/out with GPS validation. Automated payroll calculation with flexible formulas. Approval workflows (leave, salary adjustment). Mobile app (React Native) for attendance. Real-time reporting dashboard. Comprehensive audit logs. Benefits Save 70% of manual HR processing time. Reduce 90% of data entry errors. Cost only $45-70/month for 100 employees (90% cheaper than SAP/Workday). In-house development - no outsourcing costs. 3. Solution Architecture Here is the cloud architecture diagram of the system:\nAWS Services Used AWS Service Primary Function AWS Lambda Backend API logic (Node.js 20.x) API Gateway REST API endpoints, request validation Amazon DynamoDB NoSQL database (on-demand billing) AWS Cognito Authentication, SSO (Google/Microsoft), JWT tokens Amazon S3 Document storage (CV, contracts, payslips) CloudFront CDN for static assets and S3 Route 53 DNS management AWS WAF (optional Phase 2) API protection CloudWatch Logs, monitoring (free tier) Secrets Manager API keys, credentials Component Design Authentication Layer Cognito User Pools with JWT (RS256). Lambda authorizer for API Gateway. Optional MFA (SMS/TOTP) - Phase 2. API Layer AWS Lambda functions (Node.js) deployed via GitHub Actions. API Gateway REST API with resource-based routing. Rate limiting (10 requests/second). CORS configured for web/mobile. Business Logic (Lambda Functions) Employee management (CRUD, contracts, skills). Attendance tracking (check-in/out, GPS validation). Leave management (requests, approvals, balance). Payroll engine (salary calculation, tax, insurance). Performance reviews (KPI tracking). Email notifications (SES free tier). Data Layer - DynamoDB Tables Users - GSI on email Employees - GSI on department_id Departments AttendanceLogs - GSI on employee_id + date LeaveRequests - GSI on employee_id + status PayrollRecords - GSI on employee_id + month Approvals - GSI on approver_id + status Storage Layer S3 Standard for new documents (\u0026lt;30 days). S3 Lifecycle → Glacier Deep Archive (\u0026gt;90 days). Presigned URLs for secure upload/download. CloudFront distribution for static web hosting. Frontend Next.js 14 (React 18) + TypeScript - Static export. Material-UI components. Hosted on CloudFront + S3 (no server cost). Mobile app: React Native (Expo) with AsyncStorage. CI/CD Pipeline GitHub Actions workflow: Build Lambda functions → ZIP packages Deploy to Lambda via AWS CLI Update API Gateway configurations Deploy frontend to S3 Automated Jest unit tests. 4. Technical Implementation Phase 1: MVP Core (Month 1-2) Month 1:\nAWS setup (Cognito, DynamoDB tables, S3, Lambda). Authentication + Login UI. Employee CRUD APIs + admin dashboard. Month 2:\nAttendance check-in/out APIs with GPS. Mobile app MVP (React Native). Leave request workflow. Basic reporting dashboard. Phase 2: Payroll \u0026amp; Automation (Month 3-4) Month 3:\nPayroll calculation engine (Lambda). Payslip generation (PDF via Lambda layer). Approval workflows. Month 4:\nEmail notifications (SES). Audit logging to DynamoDB. Export reports (CSV). Performance optimization. Phase 3: Advanced Features (Month 5-6) Performance review module. Training tracking. Advanced analytics dashboard. Security hardening. Load testing \u0026amp; optimization. User training \u0026amp; documentation. Tech Stack Component Technology/Service Backend Node.js 20.x, AWS Lambda, AWS SDK v3 Database DynamoDB (single-table design pattern) Frontend Next.js 14, React 18, TypeScript, Material-UI v5 Mobile React Native (Expo), AsyncStorage Infrastructure as Code AWS SAM / Serverless Framework CI/CD GitHub Actions 5. Roadmap \u0026amp; Milestones Month Phase Key Deliverables 1-2 MVP Core Auth, Employee management, Attendance mobile app 3-4 Payroll \u0026amp; Automation Payroll engine, approval workflows, notifications 5-6 Advanced \u0026amp; Launch Analytics, performance reviews, UAT, go-live 6. Budget Estimation Monthly AWS Costs (Phase 1: 100 employees, ~5,000 API calls/day) Serverless Architecture - Cost Optimized Service Configuration Cost/Month AWS Lambda 150K invocations, 512MB, 500ms avg $0 ↳ Free tier: 1M requests + 400K GB-seconds/month (Within free tier) API Gateway 150K REST API requests/month $0.15 ↳ $3.50 per million after first 1M (free tier year 1) DynamoDB On-demand, 5GB storage, 1M reads, 500K writes $3.50 ↳ Storage: $1.25/GB ($6.25) + Reads: $0.25/M + Writes: $1.25/M S3 Storage 20GB documents (100 users) $0.46 S3 Requests 20K PUT, 100K GET/month $0.14 S3 Glacier (archive) 10GB old documents $0.10 CloudFront 10GB transfer, 200K requests $1.00 Route 53 1 hosted zone + 1M queries $0.90 CloudWatch Logs 2GB logs/month $0 ↳ (First 5GB free) (Within free tier) Secrets Manager 2 secrets $0.80 SES (email) 500 emails/month $0.05 Cognito \u0026lt;50K MAU $0 ↳ (Free tier) (Within free tier) Data Transfer OUT 5GB to internet $0.45 Contingency (10%) Buffer $0.75 TOTAL AWS/MONTH (100 users) ~$8.30 Costs When Scaling to 200 Users (Phase 2) Service Changes Cost/Month Lambda 300K invocations (still in free tier) $0 API Gateway 300K requests $0.30 DynamoDB 10GB, 2M reads, 1M writes $9.50 S3 + CloudFront 40GB storage, 20GB transfer $2.50 Route 53, Secrets, SES, Transfer (similar) $2.20 ElastiCache Redis cache.t3.micro (optional) $12.50 AWS WAF Basic protection (optional) $7.00 Contingency $3.40 TOTAL (200 users, with cache + WAF) ~$37.40 TOTAL (200 users, without cache/WAF) ~$17.90 Costs When Scaling to 500 Users (Phase 3) | Lambda + API Gateway | 750K invocations | $3.50 | | DynamoDB | 25GB, 5M reads, 2.5M writes | $32.50 | | S3 + CloudFront + Transfer | 100GB storage, 50GB CDN | $7.50 | | ElastiCache Redis | cache.t3.small | $25.00 | | AWS WAF | 2 rules | $8.00 | | Route 53, Secrets, SES, misc | | $3.00 | | Contingency | | $8.00 | | | | | | TOTAL (500 users) | | ~$87.50 |\nHosting Cost Summary by Phase Phase Users Cost/Month Cost/Year Phase 1 MVP 100 $8-12 ~$100-150 Phase 2 Growth 200 $18-38 ~$220-450 Phase 3 Scale 500 $88-95 ~$1,050 Development Costs (In-house team - NO outsourcing cost) Assumption: In-house team already has fixed salaries, only AWS and tools costs counted\nItem Cost AWS hosting (6 months dev/staging @ $5/mo) $30 GitHub Pro (team of 5) $0 ↳ (Can use free tier) Domain name (.com) $12/year Third-party libraries (optional) $0 TOTAL DEVELOPMENT COST ~$42 Note: Personnel costs NOT included as this is an in-house team with fixed salaries\nAnnual Operating Costs (post go-live) Item Cost/Year AWS Hosting (Phase 1: 100 users) $100-150 Third-party services (SMS for MFA - optional) $100 Domain renewal $12 TOTAL OPERATING/YEAR (Phase 1) ~$212-262 ROI Analysis (In-house project) Initial Investment:\nSetup + Dev tools: ~$42 AWS (6 months dev): ~$30 Total initial: ~$72 First Year Operating Costs:\nPhase 1 (6 months, 100 users): $60 Phase 2 (6 months, 200 users): $150 Total Year 1: ~$210 Total Year 1 Cost: ~$282\nSavings vs Alternatives:\nSAP SuccessFactors: $8-15/user/month = $9,600-18,000/year BambooHR: $6-10/user/month = $7,200-12,000/year Manual Excel: 1 FTE HR admin = $12,000/year Year 1 Savings: $6,918 - $17,718 Year 1 ROI: 2,454% - 6,281% 🚀\n7. Risk Assessment \u0026amp; Mitigation Risk Impact Probability Mitigation DynamoDB costs spike Medium Low On-demand billing, CloudWatch alarms at $30 threshold Lambda cold starts Low Medium Keep functions warm, optimize bundle size \u0026lt;1MB API Gateway rate limits Medium Low Default 10K req/s sufficient, implement caching Vendor lock-in (AWS) Medium High Use Serverless Framework for portability Team learning curve Low Medium Start with 1-2 Lambda functions, expand gradually Cost Optimization Best Practices Lambda: Bundle size \u0026lt;1MB, reuse connections, avoid cold starts. DynamoDB: Single-table design, use GSIs carefully, on-demand billing. S3: Lifecycle policies to Glacier, presigned URLs, CloudFront caching. API Gateway: Response caching (30-60s), throttling. CloudWatch: Log retention 7 days, filter unnecessary logs. 8. Expected Outcomes Technical Improvements 85% HR processes automated. Real-time dashboard with data \u0026lt; 5 seconds old. \u0026lt; 1s API response time (P95) with Lambda. 70% employees use mobile app. Zero server maintenance. Infinite scalability with serverless. Business Value HR team reduces 60% manual workload. Employee satisfaction increases 40% (self-service). 100% audit trail for compliance. Payroll accuracy 99.5%. Cost savings $6,900-17,700/year vs alternatives. Operating cost only $8-12/month for 100 users. Long-term Vision Scale to 500 users at ~$88/month cost. Integrate AI/ML (AWS Bedrock) for predictive analytics. Multi-branch operations. Potential SaaS product. 9. Conclusion HR Management System with Serverless Architecture provides:\n✅ Ultra-low cost: Only $8-12/month for 100 users Phase 1\n✅ No upfront cost: ~$72 setup, no outsourcing costs\n✅ Massive ROI: Save $6,900-17,700/year vs alternatives\n✅ Scalable: Pay-as-you-go, auto-scale to 500+ users\n✅ Zero maintenance: Serverless = no server management\n✅ Fast development: 6 months MVP → production\nThis is an ideal solution for startups/SMEs with in-house teams wanting to build a modern HR system without large investments.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Master knowledge of network security in AWS (Security Groups, NACL). Understand VPC connectivity and network connection methods. Learn about load balancing with AWS ELB and Load Balancer types. Tasks to be implemented this week: Day Tasks Start Date Completion Date Documentation Source 2 - Learn about Security Groups: + Concepts and stateful characteristics + How to create and configure rules + Apply to Elastic Network Interface 08/11/2025 08/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Network ACL (NACL): + Concepts and stateless characteristics + Differences between NACL and Security Groups + Rule evaluation mechanism from top to bottom 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Hands-on Practice: + Create Security Group for EC2 instance + Configure NACL for subnet + Compare security effectiveness between 2 methods - Learn about VPC Flow Logs 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about VPC connectivity: + VPC Peering and limitations + Transit Gateway + Site-to-Site VPN + AWS Direct Connect 08/14/2025 08/14/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about Elastic Load Balancing: + ELB concepts and types + Application Load Balancer (ALB) + Network Load Balancer (NLB) + Sticky Session and Health Check - Hands-on Practice: + Create ALB to distribute traffic + Configure Target Groups 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Gained clear understanding of Security Groups and NACL:\nSecurity Groups are stateful firewalls that only allow \u0026ldquo;allow\u0026rdquo; rules NACL is a stateless firewall applied to subnets Security Groups affect individual instances, NACL affects multiple servers in a subnet Learned how to use VPC Flow Logs to monitor IP traffic in VPC without capturing packet contents.\nUnderstood VPC connection methods:\nVPC Peering to connect 2 VPCs, does not support transitive routing Transit Gateway to connect multiple VPCs and on-premises networks Site-to-Site VPN with Virtual Private Gateway and Customer Gateway AWS Direct Connect with low latency (20-30ms) Mastered knowledge of Elastic Load Balancing:\n4 types: ALB, NLB, Classic LB, Gateway LB ALB operates at Layer 7, supports HTTP/HTTPS and path-based routing NLB operates at Layer 4, supports TCP/TLS and static IP Features: Health Check, Sticky Session, Access Logs Successfully completed hands-on practice:\nCreated Security Groups for EC2 with appropriate rules Configured NACL for subnet with logical rule ordering Set up Application Load Balancer with Target Groups Verified Load Balancer operation and traffic distribution Gained ability to compare and select appropriate security solutions and network connectivity for specific use cases.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/5-workshop/5.3-s3-vpc/","title":"Create DynamoDB Table","tags":[],"description":"","content":"Create DynamoDB Table DynamoDB is a NoSQL database that will store our tasks. Let\u0026rsquo;s create a table to store task data.\nStep 1: Navigate to DynamoDB Go to AWS Console Search for \u0026ldquo;DynamoDB\u0026rdquo; in the services search bar Click on DynamoDB Step 2: Create Table Click Create table button Configure the table: Table name: tasks Partition key: id (type: String) Table settings: Use default settings Capacity mode: On-demand (recommended for this workshop) Step 3: Create Table Scroll down and click Create table Wait for the table to be created (usually takes a few seconds) Table Structure Our DynamoDB table will have the following structure:\nid (String) - Primary key - Unique identifier for each task title (String) - Task title description (String) - Task description status (String) - Task status (e.g., \u0026ldquo;pending\u0026rdquo;, \u0026ldquo;completed\u0026rdquo;) createdAt (String) - Timestamp when task was created updatedAt (String) - Timestamp when task was last updated Next Steps Once the table is created, we\u0026rsquo;ll proceed to create Lambda functions that will interact with this table.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Python 3.13 runtime is now available in AWS Lambda This blog announces the availability of Python 3.13 as a managed runtime and container base image in AWS Lambda. You will learn about the new features in Python 3.13 including data model improvements, typing changes with PEP 702 and PEP 696, and standard library enhancements. The article explains the transition to Amazon Linux 2023, important changes for container deployments, and performance considerations for the new runtime. It also covers unavailable experimental features like free-threaded CPython and JIT compiler, and provides practical guidance on how to start using Python 3.13 in Lambda through the AWS Management Console and container images.\nBlog 2 - Implementing custom domain names for private endpoints with Amazon API Gateway This blog introduces the new support for custom domain names with private REST API endpoints in Amazon API Gateway. You will learn how to use simple and intuitive custom domain names instead of complex default URLs for your private APIs that are only accessible within your Amazon VPC. The article explains the distinction between frontend and backend integrations, connectivity types (public internet vs private VPC), and provides three different options for clients to connect to private REST API endpoints depending on VPC endpoint configuration. It also demonstrates how to share custom domain names across AWS accounts using AWS Resource Access Manager (AWS RAM) and includes practical configuration examples.\nBlog 3 - Accelerating data science innovation: How Bayer Crop Science uses AWS AI/ML services This blog presents how Bayer Crop Science leverages AWS AI/ML services to build their next-generation MLOps platform called Decision Science Ecosystem (DSE). You will learn how the company addresses the challenge of scaling genomic prediction models and managing large-scale data science operations to support regenerative agriculture. The article details their implementation of automated code documentation using Amazon Q, Lambda functions, and API Gateway, which reduces developer onboarding time by up to 70% and improves productivity by up to 30%. It also covers their integration with Amazon SageMaker for model training, the use of AWS Batch and EventBridge for documentation quality improvement, and how this solution supports their mission to increase crop yields by 50% by 2050 while restoring the environment.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Get to know First Cloud Journey members Understand fundamental AWS services and master the use of AWS Management Console \u0026amp; AWS CLI Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Meet and greet FCJ members - Read and take notes on internship rules and regulations 08/11/2025 08/11/2025 3 - Overview of AWS and its service categories (Compute, Storage, Networking, Database, etc.) 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn AWS Management Console \u0026amp; AWS CLI - Hands-on: account creation, install \u0026amp; configure AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Deep dive into Amazon EC2 (Instance types, AMI, EBS, Key Pairs, Instance Store, User Data, Metadata, Pricing models, Auto Scaling, etc.) - Related services: Lightsail, EFS, FSx, AWS MGN 08/14/2025 08/16/2025 https://cloudjourney.awsstudygroup.com/ 6 - Hands-on practice: ∘ Launch EC2 instances ∘ Connect via SSH/RDP ∘ Attach EBS volume ∘ Create snapshots \u0026amp; custom AMIs 08/15/2025 08/16/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Successfully completed 100% of the planned tasks with the following key takeaways:\nDeep understanding of Amazon EC2 as scalable virtual servers that can fully replace physical servers for almost any workload (web hosting, applications, databases, authentication, etc.). Mastered hardware configuration via Instance Types (CPU, memory, network, storage) and related concepts: Hypervisor (Nitro/KVM/HVM/PV), Placement Groups, and Availability Zones. Clear grasp of AMI (Amazon Machine Image): contains root volume, launch permissions + block device mapping; can use AWS-provided, Marketplace, or custom AMIs; backup via AMI creation or EBS snapshots. Proficient with Key Pairs: SSH authentication for Linux and encrypted Administrator password decryption for Windows instances. Thorough knowledge of Amazon EBS (Elastic Block Store): Persistent block storage independent of EC2 lifecycle, connected via private EBS network within the same AZ SSD \u0026amp; HDD families, 99.99% availability via replication across multiple nodes Multi-Attach capability on Nitro-based instances Backup via incremental snapshots stored in S3 Understood Instance Store: high-performance ephemeral NVMe storage physically attached to the host; data lost on Stop (but preserved on Reboot); ideal for cache, buffer, swap, temporary data. Learned User Data scripts (Bash/PowerShell) and Instance Metadata for automation and self-configuration. Explored EC2 pricing models and Auto Scaling: On-Demand, Reserved Instances, Savings Plans, Spot Instances Auto Scaling Groups support multiple pricing models, multi-AZ operation, and automatic registration with Elastic Load Balancers Studied complementary services: Amazon Lightsail: low-cost, simplified VPS with bundled data transfer, ideal for dev/test and light workloads EFS (Elastic File System): fully managed NFS for Linux, pay-for-use storage, supports on-premises access via DX/VPN FSx: managed Windows File Server (SMB) and FSx for Lustre, with built-in deduplication (30–50% savings) AWS Application Migration Service (MGN): continuous replication for lift-and-shift migration and DR from on-premises/physical/virtual servers to AWS Hands-on success: launched EC2 instances, connected via SSH/RDP, attached EBS volumes, created snapshots and custom AMIs. "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/5-workshop/5.4-s3-onprem/","title":"Create Lambda Functions","tags":[],"description":"","content":"Create Lambda Functions We\u0026rsquo;ll create Lambda functions to handle CRUD operations for our tasks API.\nStep 1: Create IAM Role for Lambda First, we need to create an IAM role that allows Lambda to access DynamoDB.\nGo to IAM service in AWS Console Click Roles → Create role Select AWS service → Lambda Click Next Attach policy: AmazonDynamoDBFullAccess (for this workshop) Role name: lambda-dynamodb-role Click Create role Step 2: Create Lambda Function - Create Task Go to Lambda service Click Create function Choose Author from scratch Function name: createTask Runtime: Python 3.11 Execution role: Use existing role → Select lambda-dynamodb-role Click Create function Lambda Function Code - Create Task Replace the default code with:\nimport json import boto3 import uuid from datetime import datetime dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;tasks\u0026#39;) def lambda_handler(event, context): try: body = json.loads(event[\u0026#39;body\u0026#39;]) task = { \u0026#39;id\u0026#39;: str(uuid.uuid4()), \u0026#39;title\u0026#39;: body[\u0026#39;title\u0026#39;], \u0026#39;description\u0026#39;: body.get(\u0026#39;description\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;status\u0026#39;: body.get(\u0026#39;status\u0026#39;, \u0026#39;pending\u0026#39;), \u0026#39;createdAt\u0026#39;: datetime.utcnow().isoformat(), \u0026#39;updatedAt\u0026#39;: datetime.utcnow().isoformat() } table.put_item(Item=task) return { \u0026#39;statusCode\u0026#39;: 201, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(task) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Step 3: Create Lambda Function - List Tasks Create another function: listTasks Same configuration as above Use this code: import json import boto3 dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;tasks\u0026#39;) def lambda_handler(event, context): try: response = table.scan() tasks = response[\u0026#39;Items\u0026#39;] return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(tasks) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Step 4: Create Lambda Function - Get Task Function name: getTask\nimport json import boto3 dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;tasks\u0026#39;) def lambda_handler(event, context): try: task_id = event[\u0026#39;pathParameters\u0026#39;][\u0026#39;id\u0026#39;] response = table.get_item(Key={\u0026#39;id\u0026#39;: task_id}) if \u0026#39;Item\u0026#39; not in response: return { \u0026#39;statusCode\u0026#39;: 404, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;Task not found\u0026#39;}) } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(response[\u0026#39;Item\u0026#39;]) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Step 5: Create Lambda Function - Update Task Function name: updateTask\nimport json import boto3 from datetime import datetime dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;tasks\u0026#39;) def lambda_handler(event, context): try: task_id = event[\u0026#39;pathParameters\u0026#39;][\u0026#39;id\u0026#39;] body = json.loads(event[\u0026#39;body\u0026#39;]) # Get existing task response = table.get_item(Key={\u0026#39;id\u0026#39;: task_id}) if \u0026#39;Item\u0026#39; not in response: return { \u0026#39;statusCode\u0026#39;: 404, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;Task not found\u0026#39;}) } # Update task update_expression = \u0026#34;SET updatedAt = :updatedAt\u0026#34; expression_values = { \u0026#39;:updatedAt\u0026#39;: datetime.utcnow().isoformat() } if \u0026#39;title\u0026#39; in body: update_expression += \u0026#34;, title = :title\u0026#34; expression_values[\u0026#39;:title\u0026#39;] = body[\u0026#39;title\u0026#39;] if \u0026#39;description\u0026#39; in body: update_expression += \u0026#34;, description = :description\u0026#34; expression_values[\u0026#39;:description\u0026#39;] = body[\u0026#39;description\u0026#39;] if \u0026#39;status\u0026#39; in body: update_expression += \u0026#34;, #status = :status\u0026#34; expression_values[\u0026#39;:status\u0026#39;] = body[\u0026#39;status\u0026#39;] table.update_item( Key={\u0026#39;id\u0026#39;: task_id}, UpdateExpression=update_expression, ExpressionAttributeValues=expression_values, ExpressionAttributeNames={\u0026#39;#status\u0026#39;: \u0026#39;status\u0026#39;}, ReturnValues=\u0026#39;ALL_NEW\u0026#39; ) # Get updated task updated_response = table.get_item(Key={\u0026#39;id\u0026#39;: task_id}) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(updated_response[\u0026#39;Item\u0026#39;]) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Step 6: Create Lambda Function - Delete Task Function name: deleteTask\nimport json import boto3 dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;tasks\u0026#39;) def lambda_handler(event, context): try: task_id = event[\u0026#39;pathParameters\u0026#39;][\u0026#39;id\u0026#39;] # Check if task exists response = table.get_item(Key={\u0026#39;id\u0026#39;: task_id}) if \u0026#39;Item\u0026#39; not in response: return { \u0026#39;statusCode\u0026#39;: 404, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;Task not found\u0026#39;}) } # Delete task table.delete_item(Key={\u0026#39;id\u0026#39;: task_id}) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;message\u0026#39;: \u0026#39;Task deleted successfully\u0026#39;}) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Next Steps Now that we have all Lambda functions created, we\u0026rsquo;ll configure API Gateway to connect them together.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\u0026rdquo; Event Objectives Introduce participants to AWS AI/ML services and capabilities Provide hands-on experience with Amazon SageMaker for end-to-end machine learning workflows Explore Generative AI capabilities using Amazon Bedrock Demonstrate practical applications of AI/ML in real-world scenarios Build a community of AI/ML practitioners and enthusiasts in Vietnam Speakers AWS Vietnam Solution Architects – Technical experts presenting AI/ML services and best practices AWS ML Specialists – Demonstrating SageMaker Studio and ML workflows GenAI Experts – Showcasing Amazon Bedrock capabilities and use cases Key Highlights 1. AWS AI/ML Services Overview Amazon SageMaker – End-to-end ML platform introduction Data preparation and labeling capabilities Model training, tuning, and deployment workflows Integrated MLOps capabilities for production ML systems Live Demo: SageMaker Studio walkthrough with hands-on demonstration of the interface, data exploration, and model training processes 2. Generative AI with Amazon Bedrock Foundation Models: Comparison and selection guide for Claude, Llama, and Titan models Understanding different model capabilities and when to use which model for specific use cases Prompt Engineering: Advanced techniques including Chain-of-Thought reasoning and Few-shot learning Best practices for effective prompt design to improve model responses Retrieval-Augmented Generation (RAG): Architecture and Knowledge Base integration Understanding RAG architecture components and integrating knowledge bases with foundation models Building context-aware AI applications Bedrock Agents: Multi-step workflows and tool integrations Creating intelligent agents for complex tasks with tool integration and function calling Building autonomous AI systems Guardrails: Safety and content filtering Implementing safety controls for AI applications Content filtering and moderation Responsible AI practices Live Demo: Building a Generative AI chatbot using Bedrock with step-by-step creation, knowledge base integration, and deployment Key Takeaways Technical Knowledge Gained comprehensive understanding of Amazon SageMaker as an end-to-end ML platform Learned how to prepare data, train models, and deploy ML solutions using SageMaker Understood the capabilities of Amazon Bedrock for Generative AI applications Mastered prompt engineering techniques including Chain-of-Thought and Few-shot learning Learned to implement RAG architecture for knowledge-enhanced AI applications Understood how to build Bedrock Agents for complex multi-step workflows Gained knowledge of AI safety and guardrails for responsible AI deployment Professional Development Enhanced understanding of the AI/ML ecosystem and its practical applications Improved networking skills through interactions with AWS experts and fellow participants Gained insights into career opportunities in AI/ML and Generative AI fields Learned best practices for building production-ready AI/ML solutions Applying to Work Apply SageMaker for machine learning projects in current or future work Use Amazon Bedrock to build Generative AI applications and chatbots Implement RAG architecture to enhance AI applications with domain-specific knowledge Apply prompt engineering techniques to improve AI model performance Integrate Bedrock Agents for automating complex workflows Follow responsible AI practices and implement guardrails in AI applications Share knowledge with team members about AWS AI/ML services and capabilities Event Experience Attending AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS was an enriching and transformative experience that significantly expanded my knowledge of AWS AI/ML services and Generative AI capabilities.\nLearning and Insights The workshop provided deep insights into how AWS is democratizing AI/ML for developers and organizations. I learned directly from AWS experts about practical approaches to building ML models and Generative AI applications. The live demonstrations made complex concepts accessible and actionable. Hands-on Demonstrations Experienced SageMaker Studio in action, understanding the complete ML workflow from data to deployment. Built a Generative AI chatbot using Bedrock, gaining practical experience with foundation models. Learned to implement RAG architecture for knowledge-enhanced AI applications. Networking and Community Engaged in meaningful conversations with AWS specialists, ML engineers, and AI enthusiasts. Gained valuable advice about career growth in the AI/ML and cloud ecosystem. Connected with like-minded professionals passionate about AI and machine learning. Lessons Learned AI/ML adoption requires understanding both the technical capabilities and practical applications. Generative AI is transforming how we build applications, but requires careful prompt engineering and safety considerations. RAG architecture enables AI applications to leverage domain-specific knowledge effectively. Responsible AI practices are essential for building trustworthy and safe AI systems. Continuous learning and hands-on experimentation are key to mastering AI/ML technologies. Conclusion: This event was a comprehensive introduction to AWS AI/ML services and Generative AI. It equipped me with both theoretical knowledge and practical skills to build AI/ML solutions using AWS services. The hands-on experience with SageMaker and Bedrock has inspired me to explore these technologies further in my projects and career.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in six events. Each one offered valuable experiences, opportunities to connect with industry professionals, and practical insights into cloud computing and AI technologies.\nEvent 1 Event Name: VietNam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 13:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription:\nThe event was organized by AWS Vietnam and focused on the latest trends in cloud computing, especially how cloud infrastructure supports innovation for developers and enterprises. The sessions included technical demonstrations, success stories, and panel discussions from leading cloud experts.\nOutcomes:\nI gained a deeper understanding of AWS services, cloud architecture best practices, and how organizations leverage the cloud for scalability and security. It also provided a great opportunity to network with developers and cloud professionals from various companies.\nEvent 2 Event Name: [AWS GenAI Builder Club] AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription:\nThis event explored the integration of Generative AI (GenAI) in modern software development processes. Speakers discussed AI-assisted coding, automated testing, and system optimization using machine learning tools.\nOutcomes:\nI learned how AI can transform the software engineering workflow, reduce repetitive tasks, and enhance creativity. The event inspired me to experiment with AI tools such as Amazon Bedrock and CodeWhisperer to optimize development productivity.\nEvent 3 Event Name: [AWS GenAI Builders Club] Game Day - Secret Agent(ic) Unicorns\nDate \u0026amp; Time: 14:00, November 21, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription:\nGame Day was a hands-on, team-based event designed to simulate real-world cloud operations and problem-solving scenarios. Participants worked in groups to complete technical challenges using AWS services, with a focus on automation, monitoring, and fault tolerance.\nOutcomes:\nThis interactive experience strengthened my teamwork, problem-solving, and decision-making skills under time pressure. I also improved my familiarity with AWS tools like CloudWatch, Lambda, and EC2 in practical situations.\nEvent 4 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 8:00 AM - 11:30 AM, November 15, 2025\nLocation: Bitexco Financial Tower, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nDescription:\nThis comprehensive workshop focused on AWS AI/ML services and Generative AI capabilities. The event covered Amazon SageMaker for end-to-end machine learning workflows, including data preparation, model training, and deployment. The second half explored Amazon Bedrock for Generative AI applications, covering foundation models, prompt engineering, RAG architecture, Bedrock Agents, and AI safety guardrails.\nOutcomes:\nI gained deep understanding of Amazon SageMaker as a complete ML platform and learned to build Generative AI applications using Amazon Bedrock. The hands-on demonstrations with SageMaker Studio and building a chatbot with Bedrock provided practical experience. I also learned advanced prompt engineering techniques, RAG architecture implementation, and responsible AI practices.\nEvent 5 Event Name: AWS Cloud Mastery Series #2: DevOps on AWS\nDate \u0026amp; Time: 8:30 AM - 5:00 PM, November 17, 2025\nLocation: Bitexco Financial Tower, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nDescription:\nThis comprehensive workshop focused on DevOps practices and AWS DevOps services. The morning session covered DevOps mindset and principles, CI/CD pipelines using CodeCommit, CodeBuild, CodeDeploy, and CodePipeline, as well as Infrastructure as Code with CloudFormation and AWS CDK. The afternoon session explored container services (ECR, ECS, EKS, App Runner), monitoring and observability with CloudWatch and X-Ray, and DevOps best practices with real-world case studies.\nOutcomes:\nI gained comprehensive understanding of DevOps culture and principles, learned to build CI/CD pipelines using AWS DevOps services, and mastered Infrastructure as Code with CloudFormation and CDK. The hands-on demonstrations with container services and monitoring tools provided practical experience. I also learned deployment strategies (Blue/Green, Canary), monitoring best practices, and real-world DevOps transformation approaches.\nEvent 6 Event Name: AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 8:30 AM - 12:00 PM, November 29, 2025\nLocation: Bitexco Financial Tower, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nDescription:\nThis comprehensive workshop focused on AWS Well-Architected Security Pillar and its five components. The event covered Security Foundation with core principles (Least Privilege, Zero Trust, Defense in Depth) and Shared Responsibility Model. It explored Identity \u0026amp; Access Management (IAM Identity Center, SCP, permission boundaries), Detection (CloudTrail, GuardDuty, Security Hub), Infrastructure Protection (VPC segmentation, Security Groups, WAF, Shield), Data Protection (KMS, encryption, Secrets Manager), and Incident Response playbooks with automation using Lambda and Step Functions.\nOutcomes:\nI gained comprehensive understanding of AWS Well-Architected Security Pillar and its five pillars. I learned IAM best practices, detection strategies, infrastructure protection, data protection techniques, and incident response automation. The hands-on demonstrations with IAM policy validation, detection setup, and security services provided practical experience. I also learned about security threats in cloud environments in Vietnam and how to implement security-first architecture design.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Master Amazon Simple Storage Service (S3) and its ecosystem Understand object storage concepts, storage classes, lifecycle management, security, and performance optimization Explore the AWS Snow Family and hybrid storage solutions Learn disaster recovery concepts (RTO/RPO) and backup strategies Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1–2 Deep dive into Amazon S3: architecture, durability, availability, storage classes, lifecycle policies, versioning, static website hosting, CORS, access control (ACL, Bucket Policy, IAM), Access Points, endpoints 08/18–08/19 08/19/2025 https://cloudjourney.awsstudygroup.com/ 3 S3 performance optimization, multipart upload, prefix design, event notifications, replication (CRR/SRR) 08/20/2025 08/20/2025 4 Amazon S3 Glacier (Instant Retrieval, Flexible Retrieval, Deep Archive), retrieval options (Expedited, Standard, Bulk) 08/21/2025 08/21/2025 5 AWS Snow Family (Snowball, Snowball Edge, Snowmobile) and AWS Storage Gateway (File, Volume, Tape) 08/22/2025 08/22/2025 6 Disaster recovery concepts (RTO, RPO), 4 DR strategies on AWS, AWS Backup service 08/23/2025 08/23/2025 Week 4 Achievements: Successfully completed all Week 4 objectives with the following key learnings:\nMastered Amazon S3 fundamentals:\nObject storage (not block/file), write-once-read-many (WORM) model, unlimited total capacity, max 5 TB per object Designed for 99.999999999% (11×9s) durability and 99.99% availability Data automatically replicated across minimum 3 Availability Zones in a region Supports multipart upload, event notifications, static website hosting, and CORS configuration Understood S3 Storage Classes and cost optimization:\nS3 Standard, Standard-IA, One Zone-IA, Intelligent-Tiering, Glacier Instant Retrieval, Glacier Flexible Retrieval, Glacier Deep Archive Lifecycle policies to automatically transition or expire objects Security \u0026amp; access control:\nS3 Bucket Policies, IAM Policies, S3 Access Control Lists (legacy), S3 Access Points Block Public Access, VPC Endpoints for private connectivity Advanced features:\nVersioning (protects against accidental deletion/overwrites and ransomware) Cross-Region Replication (CRR) \u0026amp; Same-Region Replication (SRR) S3 performance optimization using random prefixes to distribute objects across partitions Deep understanding of Amazon S3 Glacier:\nLow-cost archival storage with three retrieval options: Expedited (1–5 min) Standard (3–5 hours) Bulk (5–12 hours) Learned AWS Snow Family for large-scale data migration:\nSnowball (80 TB), Snowball Edge (100 TB + compute), Snowmobile (up to 100 PB per truck) Mastered AWS Storage Gateway – hybrid storage:\nFile Gateway → S3 (NFS/SMB) Volume Gateway → S3 (iSCSI, cached or stored mode, snapshot to EBS) Tape Gateway → Virtual Tape Library (S3/Glacier) Disaster Recovery concepts:\nRTO (Recovery Time Objective) – maximum acceptable downtime RPO (Recovery Point Objective) – maximum acceptable data loss Four AWS DR strategies: Backup \u0026amp; Restore, Pilot Light, Warm Standby, Multi-Site Active/Active AWS Backup – centralized backup management across EBS, EC2, RDS, DynamoDB, EFS, Storage Gateway Hands-on practice completed:\nCreated S3 buckets with versioning, lifecycle policies, static website hosting Configured bucket policies, CORS, and VPC endpoints Tested multipart upload and event triggers Explored AWS Backup console and created backup plans "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/5-workshop/5.5-policy/","title":"Configure API Gateway","tags":[],"description":"","content":"Configure API Gateway Now we\u0026rsquo;ll create a REST API in API Gateway and connect it to our Lambda functions.\nStep 1: Create REST API Go to API Gateway service Click Create API Choose REST API → Build Choose New API API name: TaskManagementAPI Description: Serverless REST API for Task Management Endpoint Type: Regional Click Create API Step 2: Create Resources In the API, you\u0026rsquo;ll see a root resource / Click Actions → Create Resource Resource name: tasks Resource path: tasks Enable API Gateway CORS Click Create Resource Step 3: Create Methods Create POST Method (Create Task) Select /tasks resource Click Actions → Create Method Select POST → Click checkmark Integration type: Lambda Function Lambda Region: Your region Lambda Function: createTask Enable Use Lambda Proxy Integration Click Save → OK (when prompted to add permissions) Create GET Method (List Tasks) Select /tasks resource Click Actions → Create Method Select GET → Click checkmark Integration type: Lambda Function Lambda Function: listTasks Enable Use Lambda Proxy Integration Click Save → OK Create GET Method (Get Single Task) Select /tasks resource Click Actions → Create Resource Resource name: {id} Resource path: {id} Enable API Gateway CORS Click Create Resource Select /tasks/{id} resource Create GET method → Connect to getTask function Create PUT method → Connect to updateTask function Create DELETE method → Connect to deleteTask function Step 4: Enable CORS Select /tasks resource Click Actions → Enable CORS Accept default settings Click Enable CORS and replace existing CORS headers Step 5: Deploy API Click Actions → Deploy API Deployment stage: New Stage Stage name: dev Stage description: Development stage Click Deploy Step 6: Get API Endpoint After deployment, you\u0026rsquo;ll see the Invoke URL. This is your API endpoint.\nExample: https://abc123xyz.execute-api.us-east-1.amazonaws.com/dev\nAPI Endpoints Your API will have the following endpoints:\nPOST /tasks - Create a new task GET /tasks - List all tasks GET /tasks/{id} - Get a specific task PUT /tasks/{id} - Update a task DELETE /tasks/{id} - Delete a task Testing the API You can test the API using:\nAPI Gateway Console - Use the \u0026ldquo;Test\u0026rdquo; feature Postman - Import the API and test endpoints curl - Use command line Browser - For GET requests Example cURL Commands # Create a task curl -X POST https://your-api-id.execute-api.region.amazonaws.com/dev/tasks \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;title\u0026#34;: \u0026#34;Learn AWS\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Complete Lambda workshop\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;}\u0026#39; # List all tasks curl https://your-api-id.execute-api.region.amazonaws.com/dev/tasks # Get a specific task curl https://your-api-id.execute-api.region.amazonaws.com/dev/tasks/{task-id} # Update a task curl -X PUT https://your-api-id.execute-api.region.amazonaws.com/dev/tasks/{task-id} \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;title\u0026#34;: \u0026#34;Updated title\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;}\u0026#39; # Delete a task curl -X DELETE https://your-api-id.execute-api.region.amazonaws.com/dev/tasks/{task-id} Next Steps Test your API thoroughly, then proceed to cleanup section to remove resources.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #2: DevOps on AWS\u0026rdquo; Event Objectives Introduce DevOps culture, principles, and benefits in modern software development Demonstrate AWS DevOps services for building CI/CD pipelines Explore Infrastructure as Code (IaC) tools: CloudFormation and AWS CDK Provide hands-on experience with container services: ECR, ECS, EKS, and App Runner Teach monitoring and observability best practices using CloudWatch and X-Ray Share DevOps best practices and real-world case studies Speakers AWS Vietnam Solution Architects – Technical experts presenting DevOps services and best practices DevOps Specialists – Demonstrating CI/CD pipelines and infrastructure automation Container Experts – Showcasing container services and orchestration strategies Guest Speakers – Sharing real-world DevOps transformation case studies Key Highlights 1. DevOps Mindset and Principles Recap of AI/ML session from previous workshop DevOps culture and principles: collaboration, automation, and continuous improvement Benefits and key metrics: DORA metrics, MTTR (Mean Time To Recovery), deployment frequency Understanding how DevOps accelerates software delivery and improves reliability 2. AWS DevOps Services – CI/CD Pipeline Source Control: AWS CodeCommit and Git strategies (GitFlow, Trunk-based development) Build \u0026amp; Test: CodeBuild configuration and testing pipelines Deployment: CodeDeploy with deployment strategies including Blue/Green, Canary, and Rolling updates Orchestration: CodePipeline automation for end-to-end CI/CD workflows Live Demo: Full CI/CD pipeline walkthrough demonstrating automated build, test, and deployment 3. Infrastructure as Code (IaC) AWS CloudFormation: Templates, stacks, and drift detection AWS CDK (Cloud Development Kit): Constructs, reusable patterns, and multi-language support Demo: Deploying infrastructure with CloudFormation and CDK Discussion: Choosing between IaC tools based on project requirements and team preferences 4. Container Services on AWS Docker Fundamentals: Microservices architecture and containerization concepts Amazon ECR: Container image storage, scanning, and lifecycle policies Amazon ECS \u0026amp; EKS: Deployment strategies, auto-scaling, and container orchestration AWS App Runner: Simplified container deployment for developers Demo \u0026amp; Case Study: Microservices deployment comparison across different container services 5. Monitoring \u0026amp; Observability CloudWatch: Metrics, logs, alarms, and dashboards for comprehensive monitoring AWS X-Ray: Distributed tracing and performance insights for microservices Demo: Full-stack observability setup with CloudWatch and X-Ray Best Practices: Alerting strategies, dashboard design, and on-call processes 6. DevOps Best Practices \u0026amp; Case Studies Deployment strategies: Feature flags and A/B testing for safe deployments Automated testing and CI/CD integration for quality assurance Incident management and postmortem processes for continuous improvement Case Studies: Real-world examples of startups and enterprise DevOps transformations Key Takeaways Technical Knowledge Gained comprehensive understanding of DevOps culture and principles and their impact on software delivery Learned to build CI/CD pipelines using AWS CodeCommit, CodeBuild, CodeDeploy, and CodePipeline Mastered Infrastructure as Code with CloudFormation and AWS CDK Understood container services: ECR, ECS, EKS, and App Runner for microservices deployment Learned monitoring and observability practices using CloudWatch and X-Ray Gained knowledge of deployment strategies (Blue/Green, Canary, Rolling) and when to use each Understood DevOps best practices including feature flags, automated testing, and incident management Professional Development Enhanced understanding of how DevOps accelerates software delivery and improves team collaboration Improved knowledge of key DevOps metrics (DORA, MTTR) for measuring success Gained insights into career pathways in DevOps and AWS certification roadmap Learned from real-world case studies of successful DevOps transformations Applying to Work Implement CI/CD pipelines using AWS DevOps services for automated deployments Use Infrastructure as Code (CloudFormation or CDK) to manage infrastructure as code Apply container services (ECS/EKS) for microservices architecture in projects Set up monitoring and observability with CloudWatch and X-Ray for production systems Implement deployment strategies (Blue/Green, Canary) for safe and reliable deployments Follow DevOps best practices including feature flags, automated testing, and incident management Share knowledge with team members about DevOps culture and AWS DevOps services Event Experience Attending AWS Cloud Mastery Series #2: DevOps on AWS was an intensive and comprehensive learning experience that significantly expanded my understanding of DevOps practices and AWS DevOps services.\nLearning and Insights The workshop provided deep insights into how DevOps culture transforms software development and operations. I learned directly from AWS experts about building robust CI/CD pipelines and infrastructure automation. The live demonstrations made complex DevOps concepts accessible and practical. Hands-on Demonstrations Experienced full CI/CD pipeline walkthrough with CodeCommit, CodeBuild, CodeDeploy, and CodePipeline. Deployed infrastructure using CloudFormation and CDK, understanding the differences and use cases. Explored container services with hands-on demos of ECR, ECS, and EKS. Set up monitoring and observability with CloudWatch and X-Ray for distributed systems. Networking and Community Engaged in meaningful conversations with AWS specialists, DevOps engineers, and cloud practitioners. Gained valuable advice about DevOps career pathways and AWS certification roadmap. Connected with professionals who have successfully implemented DevOps transformations. Lessons Learned DevOps is a culture, not just tools – it requires collaboration, automation, and continuous improvement mindset. CI/CD pipelines are essential for fast, reliable software delivery and reducing manual errors. Infrastructure as Code enables version control, repeatability, and consistency in infrastructure management. Container services provide flexibility and scalability for microservices architectures. Monitoring and observability are critical for understanding system behavior and quickly resolving issues. Deployment strategies like Blue/Green and Canary enable safe, zero-downtime deployments. Continuous learning and hands-on practice are essential for mastering DevOps practices. Conclusion: This event provided a comprehensive introduction to DevOps on AWS, covering everything from CI/CD pipelines to container services and monitoring. It equipped me with both theoretical knowledge and practical skills to implement DevOps practices using AWS services. The hands-on experience and real-world case studies have inspired me to apply DevOps principles in my projects and continue learning in this field.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Master the AWS Shared Responsibility Model Deep understanding of identity and access management on AWS Gain proficiency in IAM, Organizations, Identity Center, Cognito, KMS, and Security Hub Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1–2 Shared Responsibility Model + IAM fundamentals (Root, Users, Groups, Policies, Roles) 25–26/08/2025 26/08/2025 https://cloudjourney.awsstudygroup.com/ 3 IAM Policies (Identity-based vs Resource-based), Policy Evaluation Logic, Explicit Deny 27/08/2025 27/08/2025 4 IAM Roles, Trust Policy, STS, AssumeRole, Cross-account access, Service roles 28/08/2025 28/08/2025 5 AWS Organizations, OU, SCP, Consolidated Billing + AWS Identity Center 29/08/2025 29/08/2025 6 Amazon Cognito, AWS KMS, AWS Security Hub 30/08/2025 30/08/2025 Week 5 Achievements: Successfully completed all Week 5 objectives with strong mastery of:\nAWS Shared Responsibility Model: AWS secures the cloud, customers secure in the cloud. Responsibility varies by service type (Infrastructure → Container → Abstracted services). Root Account Protection: MFA enabled, never used for daily tasks, credentials safely stored, IAM Administrator users created instead. AWS IAM Mastery: Principals: Root, IAM Users, IAM Roles, Federated Users, AWS Services Policy types and evaluation: Explicit Deny wins, then Allow, then default Deny IAM Roles + Trust Policy + STS for temporary, least-privilege, cross-account access Cross-account delegation and service-linked roles AWS Organizations: Centralized multi-account management with OUs Service Control Policies (SCP) to set permission guardrails Consolidated Billing AWS Identity Center (formerly AWS SSO): Identity source integration (built-in, AWS Managed Microsoft AD, AD Connector, external IdP) Permission Sets → auto-provisioned IAM Roles in member accounts Amazon Cognito: User Pools for user sign-up/sign-in Identity Pools for temporary AWS credential issuance AWS KMS: Customer Managed Keys (CMK), FIPS 140-2 compliant Data Key pattern for large-scale encryption AWS Security Hub: Continuous security checking against AWS Foundational Best Practices, CIS, PCI DSS, etc. Security score and prioritized findings Hands-on completed:\nBuilt full AWS Organizations structure with SCPs Configured AWS Identity Center with Permission Sets Implemented Cognito authentication flow Encrypted resources with KMS Activated and analyzed Security Hub findings "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a Serverless REST API with AWS Lambda and API Gateway Overview In this workshop, you will learn how to build a serverless REST API using AWS Lambda and Amazon API Gateway. This architecture allows you to create scalable, cost-effective APIs without managing servers.\nYou will build a simple Task Management API that allows users to:\nCreate new tasks List all tasks Get a specific task Update a task Delete a task The API will use:\nAWS Lambda for serverless compute functions Amazon API Gateway for REST API endpoints Amazon DynamoDB for data storage AWS IAM for security and permissions Learning Objectives By the end of this workshop, you will be able to:\nCreate Lambda functions using Python Set up API Gateway REST API Integrate Lambda with API Gateway Store and retrieve data from DynamoDB Understand serverless architecture patterns Handle API errors and responses properly Prerequisites AWS Account with appropriate permissions Basic knowledge of Python programming Understanding of REST API concepts AWS CLI configured (optional but recommended) Content Workshop Overview Prerequisites and Setup Create DynamoDB Table Create Lambda Functions Configure API Gateway Testing and Cleanup "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar\u0026rdquo; Event Objectives Understand the role of Security Pillar in AWS Well-Architected Framework Learn core security principles: Least Privilege, Zero Trust, and Defense in Depth Explore the five security pillars: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response Understand Shared Responsibility Model and top cloud security threats in Vietnam Learn practical security implementation using AWS security services Gain knowledge of security best practices and incident response playbooks Speakers AWS Vietnam Solution Architects – Security experts presenting Well-Architected Security Pillar Security Specialists – Demonstrating IAM, detection, and protection services Security Engineers – Sharing real-world security implementation patterns and incident response strategies Key Highlights 1. Security Foundation Role of Security Pillar in AWS Well-Architected Framework Core principles: Least Privilege, Zero Trust, and Defense in Depth Shared Responsibility Model: Understanding security responsibilities between AWS and customers Top security threats in cloud environments in Vietnam Importance of security-first mindset in cloud architecture 2. Pillar 1 – Identity \u0026amp; Access Management Modern IAM Architecture: Users, Roles, Policies – avoiding long-term credentials IAM Identity Center: SSO (Single Sign-On) and permission sets for centralized access management SCP \u0026amp; Permission Boundaries: Multi-account security controls MFA (Multi-Factor Authentication), credential rotation, and Access Analyzer Mini Demo: Validating IAM Policy and simulating access scenarios 3. Pillar 2 – Detection Detection \u0026amp; Continuous Monitoring: CloudTrail (organization-level), GuardDuty, and Security Hub Comprehensive Logging: VPC Flow Logs, ALB/S3 logs at every layer Alerting \u0026amp; Automation: EventBridge for automated security responses Detection-as-Code: Infrastructure and rules as code for consistent security monitoring 4. Pillar 3 – Infrastructure Protection Network \u0026amp; Workload Security: VPC segmentation, private vs public placement strategies Security Groups vs NACLs: Understanding when to use each and application models WAF + Shield + Network Firewall: Multi-layered network protection Workload Protection: EC2, ECS/EKS security basics and best practices 5. Pillar 4 – Data Protection Encryption, Keys \u0026amp; Secrets: KMS key policies, grants, and rotation Encryption at-rest \u0026amp; in-transit: Implementation for S3, EBS, RDS, and DynamoDB Secrets Manager \u0026amp; Parameter Store: Patterns for secret rotation and management Data Classification \u0026amp; Access Guardrails: Protecting sensitive data with proper classification and access controls 6. Pillar 5 – Incident Response IR Playbook \u0026amp; Automation: Incident Response lifecycle according to AWS best practices Playbook Scenarios: Compromised IAM key response S3 public exposure remediation EC2 malware detection and response Response Actions: Snapshot creation, isolation, and evidence collection Auto-response: Using Lambda and Step Functions for automated incident response Key Takeaways Technical Knowledge Gained comprehensive understanding of AWS Well-Architected Security Pillar and its five components Learned Identity \u0026amp; Access Management best practices: IAM Identity Center, SCP, permission boundaries, and MFA Understood Detection strategies: CloudTrail, GuardDuty, Security Hub, and Detection-as-Code Mastered Infrastructure Protection: VPC segmentation, Security Groups, NACLs, WAF, Shield, and Network Firewall Learned Data Protection techniques: KMS, encryption at-rest/in-transit, Secrets Manager, and data classification Gained knowledge of Incident Response playbooks and automation using Lambda and Step Functions Understood Shared Responsibility Model and core security principles (Least Privilege, Zero Trust, Defense in Depth) Professional Development Enhanced understanding of security-first architecture design Improved knowledge of AWS security services and their integration Gained insights into security threats specific to cloud environments in Vietnam Learned practical security implementation patterns and incident response strategies Understood the importance of continuous monitoring and automated security responses Applying to Work Implement IAM best practices using IAM Identity Center, permission boundaries, and MFA Set up comprehensive detection with CloudTrail, GuardDuty, and Security Hub Apply Infrastructure Protection strategies: VPC segmentation, Security Groups, and WAF Implement Data Protection with KMS, encryption, and Secrets Manager Create Incident Response playbooks and automate responses using Lambda/Step Functions Follow security principles (Least Privilege, Zero Trust, Defense in Depth) in all projects Share knowledge with team members about AWS security services and best practices Event Experience Attending AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar was an intensive and comprehensive learning experience that significantly expanded my understanding of cloud security and AWS security services.\nLearning and Insights The workshop provided deep insights into the AWS Well-Architected Security Pillar and its practical implementation. I learned directly from AWS security experts about building secure cloud architectures. The live demonstrations made complex security concepts accessible and actionable. Hands-on Demonstrations Experienced IAM policy validation and access simulation scenarios. Understood detection setup with CloudTrail, GuardDuty, and Security Hub. Learned infrastructure protection strategies with VPC, Security Groups, and WAF. Explored data protection implementation with KMS and encryption. Understood incident response automation with Lambda and Step Functions. Networking and Community Engaged in meaningful conversations with AWS security specialists and security engineers. Gained valuable advice about security career pathways and AWS Security Specialty certification. Connected with professionals who have implemented security best practices in production environments. Lessons Learned Security is foundational, not an afterthought – it must be built into architecture from the start. Least Privilege, Zero Trust, and Defense in Depth are essential principles for secure cloud design. Comprehensive detection is critical for identifying and responding to security threats quickly. Infrastructure Protection requires multiple layers: network, application, and workload security. Data Protection involves encryption, key management, and proper access controls. Incident Response playbooks and automation enable rapid response to security incidents. Continuous learning and staying updated with security threats are essential for cloud security professionals. Conclusion: This event provided a comprehensive introduction to AWS Well-Architected Security Pillar, covering all five pillars from Identity \u0026amp; Access Management to Incident Response. It equipped me with both theoretical knowledge and practical skills to implement security best practices using AWS services. The hands-on demonstrations and real-world scenarios have inspired me to prioritize security in all my cloud projects and continue learning in this critical field.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at First Cloud Journey from August 12, 2025 to November 12, 2025, I had the opportunity to apply the knowledge and skills learned at school to a real-world software development environment.\nDuring this internship, I participated in the development of a web-based employee management system for a technology company.\nThis system was designed to help organizations manage employee data, attendance, and departmental information efficiently on the cloud.\nMy position in the project team was Frontend Developer, where I focused on building the user interface, integrating APIs, and ensuring responsive design and usability.\nI consistently strived to complete assigned tasks on time, adhered to the company’s workflow standards, and collaborated closely with my teammates to deliver high-quality features and improve overall system performance.\nBelow is my self-assessment based on key criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Applying frontend knowledge, using frameworks effectively, delivering UI/UX components ☐ ✅ ☐ 2 Ability to learn Ability to learn new frontend tools, frameworks, and technologies quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative to improve the interface and fix minor bugs without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing assigned UI modules and tasks on time with good quality ☐ ✅ ☐ 5 Discipline Following company schedules, coding standards, and version control guidelines ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and continuously enhance frontend performance ☐ ✅ ☐ 7 Communication Discussing requirements and updates clearly with the backend and QA teams ☐ ✅ ☐ 8 Teamwork Collaborating effectively with developers and designers in the project ☐ ✅ ☐ 9 Professional conduct Maintaining respect, cooperation, and a positive attitude in the workplace ☐ ✅ ☐ 10 Problem-solving skills Identifying UI/UX issues, debugging, and optimizing components ☐ ✅ ☐ 11 Contribution to project/team Supporting team goals by improving interface quality and suggesting usability enhancements ☐ ✅ ☐ 12 Overall General performance and development throughout the internship period ☐ ✅ ☐ Strengths Acquired hands-on experience in frontend development using React, Tailwind CSS, and integration with RESTful APIs. Developed a deeper understanding of UI/UX principles, accessibility, and responsive design. Strengthened teamwork, communication, and adaptability through active collaboration with backend developers. Gained practical experience in cloud-based project environments (AWS) and using tools like GitHub and CodePipeline. Needs Improvement Enhance problem-solving and debugging skills: Continue practicing advanced debugging techniques and performance optimization. Improve communication clarity: Deliver more concise and structured updates during daily stand-up meetings. Strengthen coding discipline: Follow project conventions more consistently, especially in documentation and version control. Explore deeper frontend topics: Learn more about advanced state management, testing, and deployment workflows. Overall Reflection Overall, I evaluate my internship performance at a Fair level.\nThrough this internship, I gained valuable real-world experience as a frontend developer, improved my technical and teamwork skills, and developed a clearer understanding of how professional cloud-based systems are built and maintained.\nAlthough there is still room for improvement in communication and code optimization, I believe this internship has laid a strong foundation for my future career in web development.\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/5-workshop/5.6-cleanup/","title":"Testing and Cleanup","tags":[],"description":"","content":"Testing and Cleanup Testing Your API Before cleaning up, make sure to test all endpoints:\nCreate Task - POST request to /tasks List Tasks - GET request to /tasks Get Task - GET request to /tasks/{id} Update Task - PUT request to /tasks/{id} Delete Task - DELETE request to /tasks/{id} Cleanup Steps To avoid incurring charges, delete all resources created during this workshop.\nStep 1: Delete API Gateway Go to API Gateway console Select your API: TaskManagementAPI Click Actions → Delete API Type the API name to confirm Click Delete Step 2: Delete Lambda Functions Go to Lambda console Delete each function: createTask listTasks getTask updateTask deleteTask For each function:\nSelect the function Click Delete Type \u0026ldquo;delete\u0026rdquo; to confirm Click Delete Step 3: Delete DynamoDB Table Go to DynamoDB console Select table: tasks Click Delete Type \u0026ldquo;delete\u0026rdquo; to confirm Click Delete Step 4: Delete IAM Role Go to IAM console Click Roles Find role: lambda-dynamodb-role Click on the role Click Delete Type the role name to confirm Click Delete Step 5: Delete CloudWatch Log Groups (Optional) Go to CloudWatch console Click Log groups Delete log groups for each Lambda function: /aws/lambda/createTask /aws/lambda/listTasks /aws/lambda/getTask /aws/lambda/updateTask /aws/lambda/deleteTask Verification After cleanup, verify that:\n✅ No API Gateway APIs exist ✅ No Lambda functions exist ✅ No DynamoDB tables exist ✅ IAM role is deleted Summary Congratulations! You have successfully:\n✅ Created a DynamoDB table for data storage ✅ Created Lambda functions for business logic ✅ Configured API Gateway REST API ✅ Integrated Lambda with API Gateway ✅ Built a complete serverless REST API What You Learned How to create and configure DynamoDB tables How to write Lambda functions in Python How to create REST APIs with API Gateway How to integrate Lambda with API Gateway How to handle HTTP requests and responses Serverless architecture patterns Next Steps Add authentication using AWS Cognito Add API rate limiting Implement input validation Add error handling and logging Deploy to production stage Set up monitoring and alerts Thank you for completing this workshop!\n"},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Master core and advanced database concepts Clearly distinguish OLTP vs OLAP, RDBMS vs NoSQL Gain full proficiency in AWS database services: RDS, Aurora, Redshift, ElastiCache Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1–2 Core DB concepts: PK, FK, Index, Partition, Query Plan, Buffer, Log, Session 01–02/09/2025 02/09/2025 https://cloudjourney.awsstudygroup.com/ 3 RDBMS vs NoSQL • OLTP vs OLAP 03/09/2025 03/09/2025 4 Amazon RDS \u0026amp; Amazon Aurora (MySQL/PostgreSQL-compatible) 04/09/2025 04/09/2025 5 Amazon Redshift – Managed Data Warehouse \u0026amp; OLAP 05/09/2025 05/09/2025 6 Amazon ElastiCache (Redis \u0026amp; Memcached) 06/09/2025 06/09/2025 Week 6 Achievements: Successfully completed all Week 6 objectives with strong mastery of:\nCore database concepts: Primary Key, Foreign Key, Index, Partitioning, Execution Plan, Buffer Pool, Transaction Log, Session\nClear differentiation between:\nRDBMS (relational, SQL, ACID) vs NoSQL (flexible schema, eventual consistency) OLTP (fast transactions, row-based) vs OLAP (complex analytics, columnar, historical data) Amazon RDS:\nFully managed relational databases (MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Aurora) Automated backups, Read Replicas, Multi-AZ failover, storage autoscaling, encryption at rest \u0026amp; in transit Amazon Aurora:\nCloud-native relational database with MySQL \u0026amp; PostgreSQL compatibility Revolutionary storage layer for high concurrent read/write performance Unique features: Backtrack, Aurora Clones, Global Database, Multi-Master Amazon Redshift:\nFully managed petabyte-scale data warehouse optimized for OLAP MPP architecture + columnar storage Leader + Compute nodes, Redshift Spectrum, concurrency scaling Amazon ElastiCache:\nManaged Redis \u0026amp; Memcached Automatic failure detection and replacement Used as caching layer in front of databases to offload read-heavy OLTP workloads Redis preferred for new applications Hands-on completed:\nDeployed RDS Multi-AZ + Read Replica Created Aurora cluster with Global Database Built Redshift cluster and ran analytical queries Deployed ElastiCache Redis and integrated with sample app "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Sharing and Feedback 1. Working Environment\nThe working environment at First Cloud Journey (FCJ) is very dynamic and inspiring.\nThe office has a professional yet comfortable atmosphere that encourages focus and creativity.\nTeam members are approachable and supportive, which makes collaboration enjoyable and effective.\nOccasionally, I think organizing more team-building sessions or internal knowledge-sharing meetups would make the working environment even more engaging.\n2. Support from Mentor / Team Lead\nMy mentor provided clear guidance and constructive feedback throughout the internship.\nInstead of giving direct answers, they encouraged me to analyze and solve problems independently, which helped me build stronger problem-solving skills.\nThe mentor also gave me opportunities to take part in real project discussions, allowing me to better understand professional workflows and decision-making processes.\n3. Relevance of Work to Academic Major\nThe tasks assigned to me were highly relevant to my studies in Information Technology.\nWorking as a Frontend Developer in a web-based employee management project helped me apply the knowledge I learned in web programming, databases, and system design.\nI also gained exposure to AWS Cloud services, which extended my understanding beyond the classroom.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I enhanced both technical and soft skills.\nTechnically, I improved in using React, Tailwind CSS, and Git, while learning to integrate APIs effectively.\nIn addition, I learned professional skills such as time management, communication in a team environment, and agile workflow practices.\nThrough weekly discussions and code reviews, I received valuable insights from experienced engineers that shaped my professional growth.\n5. Company Culture \u0026amp; Team Spirit\nThe culture at FCJ emphasizes collaboration, learning, and respect.\nEveryone, from senior engineers to interns, works together and shares knowledge openly.\nEven when the team faced tight deadlines, the members remained supportive and solution-oriented.\nThis culture helped me feel comfortable contributing ideas and motivated me to keep improving every day.\n6. Internship Policies / Benefits\nThe internship program offers flexible working hours and access to internal technical training sessions, which were extremely helpful.\nThe allowance policy was fair, and communication with the HR/admin team was smooth.\nI especially appreciated how FCJ gave interns opportunities to work on real client-oriented projects, which made the experience more meaningful.\nAdditional Questions What was the most satisfying part of your internship?\nGaining real-world experience in frontend development while being trusted to contribute directly to an active project.\nWhat could the company improve for future interns?\nProvide a short onboarding workshop at the beginning of the internship to help new interns understand tools, repositories, and deployment workflows faster.\nWould you recommend this company to other students? Why?\nYes, definitely. The internship program at FCJ offers hands-on learning, real projects, and strong mentorship that genuinely help students transition into professional developers.\nSuggestions \u0026amp; Expectations FCJ could consider organizing monthly mini hackathons or tech talks to encourage creative ideas among interns and employees. I would be happy to continue joining future programs or collaborate again in upcoming cloud or web development projects. Overall, this internship was a valuable and memorable journey that helped me bridge the gap between theory and practice while growing both technically and personally. "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Start Employee Management Web Project - Analyze requirements and design system Research RBAC (Role-Based Access Control) architecture and Authentication Set up frontend development environment Create ideas and wireframes for Employee Login page Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Analyze Employee Management Web Project requirements - Research features: RBAC, Auth, Employee Profile, Payroll, Attendance 15/09/2025 15/09/2025 2 - Research RBAC architecture and roles: Admin HR, Manager, Employee, Payroll officer, Recruiter - Learn about MFA (2FA OTP/Authenticator) 16/09/2025 16/09/2025 3 - Set up frontend development environment (React/Vue/Angular) - Install necessary libraries: UI framework, routing, state management 17/09/2025 17/09/2025 4 - Design wireframe and UI/UX for Employee Login page - Identify login methods: email/phone/SSO (Google, Microsoft) 18/09/2025 18/09/2025 5 - Create detailed plan for Employee Login page - Identify required components: LoginForm, SSO buttons, MFA input, Forgot password 19/09/2025 19/09/2025 6 - Review and finalize design - Prepare resources: icons, images, color scheme for login page 20/09/2025 20/09/2025 Week 7 Achievements: Analyzed and understood Employee Management Web Project requirements with main modules:\nRBAC + Auth: Login via email/phone/SSO, role management, MFA, Session management Employee Profile: Employee profile management, contracts, insurance, tax Attendance \u0026amp; Time Tracking: Time tracking, leave management Payroll: Salary calculation, payslips, reports Scheduling \u0026amp; Leave: Scheduling, leave requests Recruitment/ATS: Recruitment, candidate management Training \u0026amp; Performance: Training, performance evaluation Reports \u0026amp; Dashboard: Reports and dashboard for HR \u0026amp; CEO Mastered RBAC architecture with roles:\nAdmin HR: Full system management rights Manager: Manage employees in department Employee: Regular employee Payroll officer: Salary management Recruiter: Recruitment Successfully set up frontend development environment:\nInstalled framework and necessary dependencies Configured routing and state management Set up project folder structure Completed wireframe and UI/UX design for Employee Login page:\nLogin interface with email/phone SSO integration (Google, Microsoft) MFA/2FA input form \u0026ldquo;Forgot password\u0026rdquo; link Responsive design for mobile and desktop Identified components to develop:\nLoginForm component SSO authentication buttons MFA/OTP input component Forgot password modal Session management logic "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Develop Employee Login page - Build UI components Implement form validation and error handling Integrate SSO authentication (Google, Microsoft) Build MFA/2FA processing logic Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Build LoginForm component with fields: email/phone, password - Implement form validation (email format, password strength) 22/09/2025 22/09/2025 2 - Create UI for SSO buttons (Google, Microsoft) - Styling and responsive design for login page 23/09/2025 23/09/2025 3 - Implement error handling and display error messages - Build loading states and disabled states for buttons 24/09/2025 24/09/2025 4 - Create MFA/OTP input component - Implement OTP verification logic 25/09/2025 25/09/2025 5 - Build Forgot password modal/form - Implement \u0026ldquo;Remember me\u0026rdquo; checkbox and session management 26/09/2025 26/09/2025 6 - Testing and fix bugs - Optimize UI/UX, improve accessibility 27/09/2025 27/09/2025 Week 8 Achievements: Successfully completed LoginForm component with full features:\nInput fields for email/phone and password Form validation with real-time feedback Password visibility toggle Responsive design for mobile and desktop Successfully integrated SSO authentication:\nGoogle Sign-In button with OAuth flow Microsoft Sign-In button with Azure AD integration Callback handling and token management Error handling for SSO failure cases Completed MFA/OTP component:\nOTP input with 6 digits Auto-focus and paste support Resend OTP functionality Countdown timer for resend Integration with backend API Built Forgot password flow:\nModal/form to enter email/phone Reset password link sent via email Success/error messages Complete navigation flow Implemented session management:\n\u0026ldquo;Remember me\u0026rdquo; checkbox functionality Token storage (localStorage/sessionStorage) Auto-logout when session expires Device management tracking Testing and optimization:\nUnit tests for components Integration tests for authentication flow Cross-browser compatibility testing Accessibility improvements (ARIA labels, keyboard navigation) "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Complete Employee Login page and integrate with backend API Start developing Manager Dashboard page Design layout and navigation for Manager page Build basic components for Manager interface Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Integrate Employee Login with backend API - Handle authentication response and redirect logic 29/09/2025 29/09/2025 2 - Design layout for Manager Dashboard - Build navigation menu with modules: Employees, Attendance, Leave, Reports 30/09/2025 30/09/2025 3 - Create sidebar navigation component - Implement role-based menu visibility (only show menu items Manager has permission for) 01/10/2025 01/10/2025 4 - Build Manager Dashboard overview - Create widgets: Team statistics, Pending approvals, Recent activities 02/10/2025 02/10/2025 5 - Implement header component with user profile and notifications - Build breadcrumb navigation 03/10/2025 03/10/2025 6 - Testing Manager page layout - Responsive design and mobile optimization 04/10/2025 04/10/2025 Week 9 Achievements: Completed Employee Login integration with backend:\nAPI integration for authentication endpoints Token management and refresh token logic Error handling for API failure cases Success redirect based on user role Protected routes implementation Designed and built Manager Dashboard layout:\nSidebar navigation with collapsible menu Main content area with responsive grid Header with user profile dropdown Footer with company information Mobile-friendly hamburger menu Completed navigation system:\nRole-based menu items (only show modules Manager has permission for) Active route highlighting Nested menu support for sub-modules Quick access shortcuts Search functionality in menu Built Dashboard overview components:\nTeam Statistics Widget: Display employee count, attendance rate, on leave Pending Approvals Widget: List of requests needing approval (leave requests, overtime) Recent Activities Widget: Timeline of recent activities Quick Actions: Buttons for frequently used actions Implemented header and user interface:\nUser profile dropdown with avatar Notifications bell with badge count Settings and logout options Breadcrumb navigation for deep navigation Search bar in header Testing and optimization:\nResponsive design testing on various devices Performance optimization (lazy loading, code splitting) Cross-browser compatibility Accessibility improvements "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Develop Employee Management module in Manager page Build Employee List with filtering and search Implement Employee Detail view Create forms for Manager to view and update employee information Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Build Employee List component with table view - Implement pagination and sorting functionality 06/10/2025 06/10/2025 2 - Create search and filter components - Filter by: department, position, status, date range 07/10/2025 07/10/2025 3 - Build Employee Detail page - Display information: personal info, job info, contract, skills 08/10/2025 08/10/2025 4 - Create tabs in Employee Detail: Overview, Attendance, Leave History, Performance - Implement tab navigation 09/10/2025 09/10/2025 5 - Build form for Manager to view/edit employee information (based on permissions) - Implement workflow approval for sensitive information changes 10/10/2025 10/10/2025 6 - Testing Employee management module - Fix bugs and optimize performance 11/10/2025 11/10/2025 Week 10 Achievements: Completed Employee List component:\nData table with columns: Name, Employee ID, Department, Position, Status, Actions Pagination with page size options (10, 25, 50, 100) Sorting by columns Row selection with checkbox Bulk actions (export, bulk update) Responsive table with horizontal scroll on mobile Implemented search and filtering:\nSearch bar: Search by name, employee ID, email, phone Department filter: Dropdown with multi-select Position filter: Filter by job title Status filter: Active, On leave, Terminated, Alumni Date range filter: Filter by join date Advanced filters: Modal with multiple options Save filter presets: Save frequently used filter sets Built Employee Detail page:\nPersonal Information Section: Avatar upload and display Full name, DOB, ID Card/Passport Contact information (email, phone, address) Emergency contact Job Information Section: Employee ID, Department, Position, Level Join date, Probation period Direct manager, Team members Contract Information: Contract type, Start/End date Contract file viewer/download Contract history Skills \u0026amp; Certifications: Skills list with proficiency level Certifications with expiry dates Training history Implemented tabbed interface:\nOverview tab: Employee information overview Attendance tab: Attendance history, OT hours, late/early records Leave History tab: Leave history, remaining leave balance Performance tab: KPI scores, performance reviews, goals Smooth tab switching with animation Built edit forms with permission control:\nRead-only mode for sensitive information (salary, tax info) Edit mode with validation Workflow approval for important changes Audit log tracking for all changes Confirmation dialogs before save Testing and optimization:\nUnit tests for components Integration tests for CRUD operations Performance testing with large datasets Memory leak detection and fixes "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Develop Leave Management and Approval module in Manager page Build Leave Request management Implement Approval workflow Create Attendance tracking and reports for Manager Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Build Leave Request List with filters - Implement status badges: Pending, Approved, Rejected, Cancelled 13/10/2025 13/10/2025 2 - Create Leave Request Detail modal/page - Display information: employee, dates, reason, attachments, approval workflow 14/10/2025 14/10/2025 3 - Implement Approval actions: Approve, Reject, Request more info - Build approval comments and notifications 15/10/2025 15/10/2025 4 - Build Attendance Overview for team - Calendar view to see employee attendance 16/10/2025 16/10/2025 5 - Create Attendance Reports: daily, weekly, monthly - Export reports to Excel/PDF 17/10/2025 17/10/2025 6 - Testing Leave and Attendance modules - Fix bugs and improve UX 18/10/2025 18/10/2025 Week 11 Achievements: Completed Leave Request Management:\nLeave Request List: Table view with columns: Employee, Leave Type, Dates, Duration, Status, Submitted Date Status badges with color coding (Pending=yellow, Approved=green, Rejected=red) Quick filters: My Team, Pending Approval, This Month, This Year Sort by date, employee, status Bulk approve/reject functionality Leave Request Detail: Employee information card Leave details: type, start/end date, duration, reason Attachments viewer (medical certificates, documents) Approval workflow visualization (current step, approvers) Comments/notes section Action buttons: Approve, Reject, Request Info, View History Implemented Approval Workflow:\nApproval Actions: Approve with optional comments Reject with required reason Request more information Delegate approval (if has permission) Workflow Visualization: Step-by-step approval process Current approver highlighting Timeline view of approval history Email notifications for each step Comments System: Threaded comments @mention support File attachments in comments Comment history with timestamps Built Attendance Overview:\nTeam Calendar View: Monthly calendar with color coding Legend: Present, Absent, On Leave, Holiday Click on date to see details Filter by employee or department Attendance Statistics: Present rate, Absent rate, Late arrivals Overtime hours summary Top performers and attendance issues Quick Actions: Mark attendance manually (if has permission) Add notes for attendance records Export attendance data Created Attendance Reports:\nReport Types: Daily report: attendance of the day Weekly report: weekly summary Monthly report: monthly report with charts Custom date range report Report Features: Filter by department, employee, date range Group by department or employee Charts and graphs (bar, line, pie) Summary statistics Export Functionality: Export to Excel with formatting Export to PDF with charts Email report functionality Schedule automated reports Implemented Notifications:\nReal-time notifications for new leave requests Email notifications for approvals/rejections In-app notification center Notification preferences settings Testing and optimization:\nUnit tests for approval workflow Integration tests for leave management Performance testing with large datasets User acceptance testing with Manager users "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Complete Manager Dashboard with Reports and Analytics Optimize performance and UX Comprehensive testing and bug fixes Prepare demo and documentation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Build Reports \u0026amp; Analytics module - Create dashboard charts: headcount, turnover rate, attendance trends 20/10/2025 20/10/2025 2 - Implement custom report builder - Allow Manager to create custom reports with filters and date ranges 21/10/2025 21/10/2025 3 - Optimize performance: lazy loading, code splitting, caching - Optimize API calls and reduce bundle size 22/10/2025 22/10/2025 4 - Comprehensive testing: unit tests, integration tests, E2E tests - Cross-browser testing and mobile device testing 23/10/2025 23/10/2025 5 - Fix bugs and improve UX based on feedback - Accessibility improvements and keyboard navigation 24/10/2025 24/10/2025 6 - Prepare demo presentation - Write documentation: user guide, technical docs, API integration guide 25/10/2025 25/10/2025 Week 12 Achievements: Completed Reports \u0026amp; Analytics module:\nDashboard Charts: Headcount chart by department and time Turnover rate with trend analysis Attendance trends (daily, weekly, monthly) Leave utilization rate Overtime hours distribution Performance metrics overview Interactive Charts: Click to drill-down into details Hover tooltips with detailed information Zoom and pan functionality Export charts as images Custom Report Builder: Drag-and-drop report designer Select data fields, filters, date ranges Choose chart types (bar, line, pie, table) Save and share reports Schedule automated report generation Performance Optimization:\nCode Optimization: Lazy loading for routes and components Code splitting with dynamic imports Tree shaking to reduce bundle size Memoization for expensive calculations API Optimization: Request caching with React Query/SWR Debouncing for search inputs Pagination and infinite scroll Optimistic updates for better UX Bundle Size Reduction: Bundle analysis and optimization Remove unused dependencies Compress images and assets Gzip compression for production Comprehensive Testing:\nUnit Tests: Component testing with React Testing Library Utility functions testing Coverage \u0026gt; 80% Integration Tests: API integration testing Authentication flow testing CRUD operations testing E2E Tests: Critical user flows with Cypress/Playwright Login flow, Employee management, Leave approval Cross-browser Testing: Chrome, Firefox, Safari, Edge Mobile browsers (iOS Safari, Chrome Mobile) Device Testing: Desktop (1920x1080, 1366x768) Tablet (iPad, Android tablets) Mobile (iPhone, Android phones) Bug Fixes and UX Improvements:\nFixed 50+ bugs from testing phase Improved error messages and user feedback Enhanced loading states and skeletons Better form validation messages Improved accessibility: ARIA labels for screen readers Keyboard navigation support Focus management Color contrast improvements Documentation:\nUser Guide: Step-by-step guides for features Screenshots and annotations FAQ section Technical Documentation: Architecture overview Component library documentation API integration guide Deployment guide Demo Preparation: Demo script with key features Sample data setup Presentation slides Project Summary:\nCompleted Features: ✅ Employee Login page with SSO and MFA ✅ Manager Dashboard with navigation ✅ Employee Management module ✅ Leave Request and Approval workflow ✅ Attendance tracking and reports ✅ Reports \u0026amp; Analytics with custom report builder Technologies Used: Frontend framework (React/Vue/Angular) UI component library State management Routing and authentication Chart libraries for analytics Key Learnings: RBAC implementation best practices Complex form handling and validation Performance optimization techniques Testing strategies for large applications UX design for enterprise applications "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/3-blogstranslated/3.4-blog4/","title":"","tags":[],"description":"","content":" "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/3-blogstranslated/3.5-blog5/","title":"","tags":[],"description":"","content":" "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/3-blogstranslated/3.6-blog6/","title":"","tags":[],"description":"","content":" "},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://tranngochai10.github.io/AWS_Proposal/en/tags/","title":"Tags","tags":[],"description":"","content":""}]